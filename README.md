# Любительский перевод статьи "Emotion recognition and artificial intelligence: A systematic review (2014–2023) and research recommendations"
# Ссылка на статью: https://www.sciencedirect.com/science/article/pii/S1566253523003354?via%3Dihub
## Распознавание эмоций и искусственный интеллект: Систематический обзор (2014-2023 гг.) и рекомендации по исследованиям

### Содержание

1. [Аннотация](#аннотация)
2. [Введение](#введение)
   - 2.1. [Парадигмы эмоций](#парадигмы-эмоций)
     - 2.1.1. [Теория дискретных эмоций](#теория-дискретных-эмоций)
     - 2.1.2. [Многомерная теория эмоций](#многомерная-теория-эмоций)
3. [Модальности распознавания эмоций](#модальности-распознавания-эмоций)
   - 3.1. [Анкеты](#анкеты)


### Аннотация
Распознавание эмоций - это способность точно определять эмоции человека по многочисленным источникам и модальностям, используя опросники, физические и физиологические сигналы. В последнее время распознавание эмоций привлекает к себе внимание благодаря разнообразным областям применения, таким как аффективные вычисления, здравоохранение, взаимодействие человека и робота и маркетинговые исследования. В данной статье представлен полный и систематический обзор методов распознавания эмоций, существующих в настоящее десятилетие. В статье рассматривается распознавание эмоций с помощью физических и физиологических сигналов. К физическим сигналам относятся речь и выражение лица, а к физиологическим - электроэнцефалограмма, электрокардиограмма, гальваническая реакция кожи и слежение за глазами. В статье представлено введение в различные модели эмоций, стимулы, используемые для вызова эмоций, и история существующих автоматизированных систем распознавания эмоций. В статье рассматривается всесторонний поиск и сканирование известных наборов данных с последующим определением критериев для анализа. После тщательного анализа и обсуждения мы отобрали 142 журнальные статьи, используя рекомендации PRISMA. В обзоре представлен подробный анализ существующих исследований и доступных наборов данных по распознаванию эмоций. В обзоре также представлены потенциальные проблемы в существующей литературе и направления будущих исследований.

### 1. Введение
Эмоции - это динамичное когнитивное и физиологическое состояние, которое развивается в ответ на такие факторы, как опыт, мысли или взаимодействие с людьми. Она включает в себя субъективный опыт, когнитивные процессы, поведенческие влияния, физиологические реакции и коммуникацию. Поэтому распознавание эмоций имеет решающее значение для таких областей применения, как маркетинг, взаимодействие человека и робота, здравоохранение, мониторинг психического здоровья и безопасность [1]. Изучение эмоций в здравоохранении включает в себя обширные неврологические расстройства, такие как нарушения сна [2], шизофрения [3], оценка качества сна [4] и болезнь Паркинсона [5]. Эмоции человека могут играть ключевую роль в выявлении таких физиологических состояний, как усталость [6], сонливость [7], депрессия [3] и боль [8]. Эксперты также предположили, что вариации эмоций имеют большое значение для изучения аутистического спектрального расстройства [9], синдрома дефицита внимания с гиперактивностью [10] и панического расстройства [11]. Изучение человеческих эмоций также имеет решающее значение для взаимодействия человека и робота и оценки "мозг-компьютер", когда машины проектируются так, чтобы вести себя как люди для различных приложений [1]. Поэтому детальное изучение человеческих эмоций и автоматизированное распознавание человеческих эмоций крайне важно.
#### 1.1. Парадигмы эмоций
Разные участки мозга вызывают разные эмоции [12]. Существует три типа эмоциональных реакций: реактивные, гормональные и автоматические [13]. Согласно психологии, эмоции - это реакция на стимулы, связанная с качественными физиологическими изменениями [13]. Два основных подхода, используемых для изучения природы эмоций, - дискретный метод и многомерный подход [13].
#### 1.1.1. Теория дискретных эмоций
Согласно этой теории, эмоции представляют собой различные и дискретные категории, каждая из которых имеет свой ансамбль когнитивных, психологических и поведенческих факторов. Эмоции могут быть положительными или отрицательными. По мнению сторонников этой гипотезы, существует несколько фундаментальных эмоций, которые общепризнаны в разных культурах. Существует шесть основных эмоций, а именно: счастье, печаль, гнев, удивление, страх и разочарование [14]. Роберт Плутчик представил всеобъемлющую эмоциональную модель, названную колесом эмоций Плутчика [15]. Колесо Плутчика состоит из восьми эмоций, а именно: страха, радости, грусти, доверия, гнева, удивления, предвкушения и отвращения. Другие сопутствующие эмоции, которые объединяют эти восемь основных эмоций, определяются по позиционной интенсивности. На рис. 1 представлен обзор колеса эмоций Плутчика [15].
![|700](https://i.imgur.com/sKeaW4I.png)
![|700](https://i.imgur.com/LxaHLmc.png)
#### 1.1.2. Многомерная теория эмоций
Многомерный подход к эмоциям признает, что эмоции сложны и подвержены влиянию множества элементов, таких как личный опыт, культурный фон и индивидуальные особенности. Он дает основу для понимания богатства и сложности эмоционального опыта и позволяет более глубоко исследовать эмоциональные состояния. Модель эмоционального пространства подразделяется на 2-мерную (2D) и 3-мерную (3D). В двухмерной модели эмоционального пространства эмоции делятся на валентность (V), которая может быть положительной (Pos) или отрицательной (Neg), и возбуждение (A), то есть высокая активация или низкая активация. Двухмерная модель эмоционального пространства Рассела, которая отображает валентность и возбуждение, показана на рис. 2 [16].
Аналогичным образом 3D-модель эмоционального пространства отображает различные непрерывные измерения, такие как V (Pos или Neg), возбуждение (высокая или низкая активация) и доминирование (D) (чувство контроля или чувство контроля). Модель трехмерного эмоционального пространства, предложенная Мехрабианом и Расселом, показана на рис. 3 [17].
![|600](https://i.imgur.com/s3Si6yH.png)
### 2. Модальности распознавания эмоций
Определение эмоций - это техника, используемая для извлечения человеческих эмоций. На протяжении многих лет для изучения человеческих эмоций применялись различные методы. Эти методы в целом делятся на три категории, а именно: анкетные, физические и физиологические, как показано на рис. 4.
#### 2.1. Анкеты
Опросники и самоотчеты предназначены для того, чтобы люди начали задумываться о различных компетенциях эмоционального интеллекта применительно к ним. Были разработаны различные методики, основанные на ручной оценке эмоций, включая шкалу позитивного и негативного аффекта (PANAS) [18], манекен самооценки (SAM) [19], фото-графический аффектометр (PAM) [20] и метод выборки опыта (ESM) [21]. PANAS - это психологическая методика для оценки и измерения положительных и отрицательных эмоций человека. Опросник PANAS состоит из двух разделов: шкала позитивного аффекта и шкала негативного аффекта [18]. SAM - это невербальный пиктографический метод оценки, который непосредственно оценивает валентность, возбуждение и доминирование, связанные с эмоциональной реакцией человека на широкий спектр стимулов [19]. PAM - это новая техника измерения аффекта, в которой пользователи выбирают из большого числа фотографий ту, которая лучше всего соответствует их текущему настроению [20]. ESM - это исследовательская техника, используемая в психологии и смежных областях для сбора данных в реальном времени об опыте, поведении и психологическом состоянии людей в их естественной среде. Она направлена на получение мгновенных или близких к реальному времени оценок опыта и контекста участников [21].
#### 2.2. Физические сигналы
Физические сигналы для распознавания эмоций включают в себя мимику, речь, текст, жесты и позы тела [22]. Речь и мимика - наиболее часто используемые механизмы распознавания эмоций среди физических сигналов [22]. В связи с этим мы решили ограничить наше обзорное исследование только физическими действиями, основанными на речи и мимике.
#### 2.3. Физиологические сигналы
Физиологические сигналы являются наиболее широко используемым источником для идентификации эмоций. Преимущество физиологических сигналов заключается в том, что они активируются непреднамеренно, поэтому не могут легко контролироваться испытуемым. Среди других преимуществ - эффективный и недорогой сбор данных, меньшее количество ошибок, связанных со светом и тенью, и меньшее вторжение в частную жизнь пользователя [22-24]. Электроэнцефалограмма (ЭЭГ), электрокардиограмма (ЭКГ), электромиограмма (ЭМГ), гальваническая реакция кожи (ГСР), дыхание (РСП), температура кожи, фотоплетизмография и слежение за глазами (СТ) - наиболее часто используемые физиологические сигналы для распознавания эмоций [22]. Среди физиологических сигналов наиболее часто используемыми модальностями для обнаружения человеческих эмоций являются ЭЭГ, ГСР, ЭКГ и ЭТ. Поэтому эти четыре физиологические модальности были включены в наш обзорный анализ.
![|500](https://i.imgur.com/rABFcMS.png)

### 3. Обзор систем автоматического распознавания эмоций
Автоматические системы распознавания эмоций включают в себя несколько этапов для точного предсказания эмоциональных состояний. Схематическое изображение этапов работы автоматизированной системы распознавания эмоций представлено на рис. 5. Краткое обсуждение каждого этапа представлено ниже:
#### 3.1. Источник
Этот первый шаг относится к части тела, используемой для измерения реакции на различные входные сигналы. Поскольку в нашем обзоре рассматриваются два физических сигнала (речь и выражение лица) и четыре физиологических сигнала (ЭЭГ, ЭКГ, GSR и ЭТ), поэтому источники сбора ограничены глазами, речью, мозгом, сердцем, кожей и лицом.
#### 3.2. Стимулы
Стимулы - это любые предметы, события или условия, которые заставляют какой-либо орган, например человека или животное, реагировать или отвечать. Стимулы широко используются в психологии и исследованиях, чтобы вызвать реакцию или поведение для изучения и понимания различных психологических процессов. Стимулы могут включать ситуации, сценарии или социальные взаимодействия, которые вызывают эмоциональные, когнитивные или поведенческие реакции. Известными стимулами, вызывающими целевые эмоции, являются виртуальная реальность (VR), изображения, видеоигры, музыка, аудио/видеоклипы, аудиозаписи и/или видео [25-27]. В зависимости от типа стимула вызываются различные эмоции, которые ранжируются вручную с помощью опросника SAM, PANAS, PAM, ESM или других подобных методик.
#### 3.3. Входные сигналы
Для эффективного анализа входные сигналы подвергаются предварительной обработке. Под предварительной обработкой понимаются этапы или процедуры, выполняемые с исходными данными перед анализом или дальнейшей обработкой. Предварительная обработка очень важна для анализа данных, поскольку она улучшает их качество, уменьшает шум или постороннюю информацию, а также подготавливает данные для эффективного анализа и моделирования. Конкретные этапы предварительной обработки определяются характером данных и целями исследования. Как правило, предварительная обработка включает в себя очистку данных (удаление артефактов и других источников шума), интеграцию данных, преобразование данных, выборку данных и масштабирование данных.
#### 3.4. Извлечение признаков
В анализе данных и машинном обучении (ML) извлечение признаков означает преобразование исходных данных в набор релевантных и репрезентативных характеристик, которые могут быть использованы для дальнейшего моделирования. Она направлена на извлечение из данных важной информации или паттернов, которые отражают ключевые черты или свойства основного явления. Цель извлечения признаков - найти и выбрать подмножество признаков, которые наилучшим образом отражают тонкие детали в данных, отбрасывая при этом избыточные или ненужные данные. Эта процедура уменьшает размерность данных, делая их более понятными и пригодными для анализа или моделирования. К наиболее распространенным признакам относятся статистические признаки, нелинейные признаки, признаки в частотной области, энтропийные признаки, признаки на основе временных частот, признаки на основе изображения, фрактальные размеры, нелинейное разложение, признаки, специфичные для конкретной области, и признаки глубокого обучения (DL).
#### 3.5. Выбор признаков
Процесс выбора подмножества релевантных характеристик из набора признаков, присутствующих в наборе данных, называется отбором признаков. В ходе него пытаются выбрать наиболее дискриминационные и информативные признаки, которые вносят наибольший вклад в анализ или прогнозирование, избегая при этом дублирования или ненужных признаков. Выбор признаков имеет решающее значение, поскольку он может ускорить вычисления, уменьшить перебор, повысить интерпретируемость и улучшить производительность модели. Наиболее распространенные методы отбора признаков включают уменьшение размерности (анализ главных компонент или анализ независимых компонент), статистический или унивариативный анализ (тест хи-квадрат, ANOVA или корреляция), методы регуляризации (Lasso (регуляризация L1) и Ridge (регуляризация L2)), алгоритмы отбора признаков или методы обертки (рекурсивное исключение признаков, последовательный отбор признаков, методы на основе деревьев или ансамблей).
#### 3.6. Классификация
Это важный шаг в автоматизированной системе обнаружения, который используется для классификации значений переменных на последующие классы. Он предполагает принятие решений с использованием методов ML или DL. К методам ML относятся, в частности, метод опорных векторов (SVM), k-nearest neighbors (KNN), дерево решений (DT), искусственная нейронная сеть (ANN), случайный лес (RF), логистическая регрессия, линейный дискриминантный анализ - одни из наиболее широко используемых методов. Сверточная нейронная сеть (CNN), сети с долговременной и кратковременной памятью (LSTM), глубокие нейронные сети (DNN), многослойные перцептроны (MLP), рекуррентные нейронные сети (RNN), генеративные состязательные сети (GAN), управляемые рекуррентные блоки, самоорганизующиеся карты, глубокое обучение с подкреплением, глубокое обучение с переносом, автоэнкодеры (AE), трансформаторы и глубокие сети убеждений (DBN) - вот некоторые из современных моделей DL.
![|900](https://i.imgur.com/k1lSuDE.png)

#### 3.7. Model evaluation
Качество и эффективность модели ML или классификации оцениваются с помощью показателей эффективности. Эти показатели дают числовую оценку эффективности модели в отношении предсказаний и обобщенности на новые данные. На выбор показателей эффективности влияют конкретная задача, тип данных и требуемые стандарты оценки. Некоторые известные показатели успешности ML/DL-моделей - это точность (ACC), отзыв, специфичность, точность, матрица путаницы, площадь под кривой операционной характеристики приемника (AUC-ROC) и оценка F-1.
### 4. Мотивация и основные моменты обзорного исследования
За последнее десятилетие было опубликовано несколько обзорных статей, посвященных распознаванию эмоций и принятию решений. Мы провели всесторонний поиск, просканировав соответствующие обзорные статьи, опубликованные в последнее время, и выявили существенные ограничения, прежде чем приступить к разработке нашего систематического обзора, как показано на рис. 6.
#### 4.1. Существующие обзорные исследования по распознаванию эмоций
Хаснул и др. [28] представили обзор по распознаванию эмоций на основе ЭКГ и их применению. В их обзоре не использовались рекомендации PRISMA и рассматривались только ЭКГ-сигналы. Авторы также обсуждают области применения, ограниченные здравоохранением, и мало говорят о проблемах и будущих направлениях. Бота и другие [29] провели всесторонний обзор по распознаванию эмоций с помощью физиологических сигналов и методов ML. В их обзоре не была применена стратегия систематического обзора с использованием рекомендаций PRISMA. В обзоре не рассматривались области применения, было представлено ограниченное обсуждение и будущие направления.
Сингх и Гоэл [30] представили системно-тематический обзор распознавания эмоций с помощью речевых сигналов, следуя рекомендациям PRISMA. Их метод охватывает применение методов ML и DL, но не охватывает проблемы исследования и всесторонние направления исследований. Камбл и Сенгупта [31] представили обзор по распознаванию эмоций с помощью сигналов ЭЭГ, не следуя рекомендациям PRISMA. Они представили подробный анализ методов извлечения признаков и принятия решений с использованием методов ML и DL. В их обзоре не рассматривались проблемы и будущие направления исследований. Чжан и другие [32] представили обзор сигналов ЭЭГ и методов ML для распознавания эмоций без соблюдения рекомендаций PRISMA. Авторы представили всестороннее исследование существующих методов, открытых проблем и будущих направлений. Adyapady и Annappa [33] представили подробный обзор распознавания эмоций на основе изображений лица с использованием методов ML и DL. Их метод обзора распознавания эмоций не связан с рекомендациями PRISMA. Авторы обсудили различные методы, наборы данных и несколько приложений для распознавания эмоций. Ба и Ху [34] провели систематический обзор, следуя рекомендациям PRISMA, по распознаванию эмоций с помощью носимых устройств в образовании. Их обзор показал, что портативные и точные носимые устройства, использующие сигналы электрокожной активности и сердечного ритма, широко распространены для распознавания эмоций в образовании.
#### 4.2. Мотивация текущего обзорного исследования
Человеческие эмоции являются важными маркерами для различных состояний и анализа поведения. В последнее время было проведено несколько обзорных исследований, посвященных многочисленным приложениям и технологиям распознавания. После всестороннего анализа литературы, посвященной обзорным статьям по распознаванию человеческих эмоций, были выявлены следующие недостатки.
- Многие исследования по распознаванию эмоций были проведены без соблюдения рекомендаций PRISMA [28,29,31-33].
- Большинство опубликованных ранее обзорных статей по распознаванию эмоций были посвящены одной модальности - физио-логическим сигналам, речи или изображениям лица [28,31-33].
- Исследования распознавания эмоций по физиологическим сигналам ограничиваются сигналами ЭЭГ или методами ML [31,32].
- Небольшое обсуждение проблем исследования и приложений [28-33].
- Ограниченные направления будущих исследований [28-33].
Вышеперечисленные пробелы побудили нас написать всеобъемлющий и системный обзор распознавания эмоций с использованием различных модальностей. Мы также сосредоточились на подробном обсуждении наборов данных, методов извлечения признаков и включении искусственного интеллекта (ИИ). В нашем обзоре представлен подробный анализ и всестороннее описание существующих методов извлечения признаков и классификации. Кроме того, в нашем обзоре подробно рассмотрены существующие пробелы в исследованиях, потенциальные области применения и направления будущих исследований в области распознавания эмоций.
![|800](https://i.imgur.com/pfyuZAG.png)

#### 4.3. Основные черты нашего систематического обзора
Уникальность и основные особенности нашего обзора перечислены ниже и показаны на рис. 7:
1. Комплексное использование баз данных: В нашем обзоре использована комплексная стратегия поиска в различных базах данных. Авторы просканировали известные базы данных, включая Web of Science, MEDLINE, PubMed/PubMed Central, IEEE Explore, Sco- pus, Wiley и другие, чтобы отобрать наиболее релевантные исследования.
2. Систематический обзор: В разработанном нами обзоре соблюдены строгие рекомендации PRISMA по отбору релевантных научных статей.
3. Временной интервал: Для сканирования и отбора статей, включенных в обзор, мы рассматривали временной промежуток в 10 лет.
4. Мультимодальное распознавание эмоций: Мы учитывали физио- логические сигналы (ЭЭГ, ЭКГ, ЭТ и ГСР) и физическую активность (речь и выражение лица). Кроме того, мы использовали ИИ, состоящий из методов ML и DL, применяемых для распознавания эмоций.
5. Разнообразные модели эмоций: Мы включили в обзор статьи о разнородных и многомерных эмоциональных моделях. Кроме того, мы ограничили поиск статей рецензируемыми журналами.
Рис. 7. Основные моменты и ключевые положения, включенные в метод обзора по распознаванию эмоций.
6. Наборы данных: Мы также представили всесторонний анализ доступных наборов данных, используемых для распознавания эмоций с помощью различных модальностей.
7. Анализ: Мы представили подробный анализ и обсуждение существующих исследований, включенных в данный обзор.
8. Проблемы и будущие направления: Мы определили текущие проблемы, представили подробное обсуждение и будущие направления исследований. Мы также рассмотрели области применения распознавания эмоций в различных областях.
![](https://i.imgur.com/Z9zMgSw.png)
### 5. Метод обзора
В настоящем систематическом обзоре использовались рекомендации по составлению отчетов для систематических обзоров и мета-анализов (PRISMA) [35]. Протокол обзора включает стратегии поиска, критерии отбора, стандарты отбора и извлечения данных. Подробности стратегий поиска, отбора и извлечения данных рассматриваются в следующих подразделах:

<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>

<table>
    <tr>
        <td rowspan="23">Включение</td>
        <td>(i) распознавание эмоций</td>
    </tr>
    <tr>
        <td>(ii) классификация эмоций с использованием сигналов ЭЭГ</td>
    </tr>
    <tr>
        <td>(iii) распознавание эмоций и сигналы ЭЭГ</td>
    </tr>
    <tr>
        <td>(iv) обнаружение эмоций с помощью сигналов ЭЭГ</td>
    </tr>
    <tr>
        <td>(v) распознавание эмоций с использованием физиологических сигналов</td>
    </tr>
    <tr>
        <td>(vi) обнаружение эмоций с помощью сигналов ЭКГ</td>
    </tr>
    <tr>
        <td>(vii) распознавание эмоций по сигналам ЭКГ</td>
    </tr>
    <tr>
        <td>(viii) автоматическое распознавание эмоций по физиологическим сигналам</td>
    </tr>
    <tr>
        <td>(ix) машинное обучение и глубокое обучение для распознавания эмоций</td>
    </tr>
    <tr>
        <td>(x) классификация электрокардиограммы и эмоций</td>
    </tr>
    <tr>
        <td>(xi) гальваническая реакция кожи для распознавания эмоций</td>
    </tr>
    <tr>
        <td>(xii) автоматизированная классификация эмоций с использованием GSR</td>
    </tr>
    <tr>
        <td>(xiii) отслеживание глаз и классификация эмоций</td>
    </tr>
    <tr>
        <td>(xiv) обнаружение эмоций с помощью слежения за глазами</td>
    </tr>
    <tr>
        <td>(xv) автоматизированное определение эмоций и слежение за глазами</td>
    </tr>
    <tr>
        <td>(xvi) речь и классификация эмоций</td>
    </tr>
    <tr>
        <td>(xvii) распознавание эмоций на основе речи</td>
    </tr>
    <tr>
        <td>(xviii) автоматизированное распознавание эмоций с помощью речи</td>
    </tr>
    <tr>
        <td>(xix) изображения лиц и классификация эмоций</td>
    </tr>
    <tr>
        <td>(xx) автоматизированная система для классификации эмоций с использованием изображений лиц</td>
    </tr>
    <tr>
        <td>(xxi) эмоции и изображения лиц</td>
    </tr>
    <tr>
        <td>(xxii) автоматизированное распознавание человеческих эмоций</td>
    </tr>
    <tr>
        <td>(xxiii) распознавание эмоций на основе глубокого обучения с использованием изображений лиц</td>
    </tr>
    <tr>
        <td rowspan="8">Исключение</td>
		<td>(i) расширенные или повторяющиеся статьи </td>
    </tr>
    <tr>
		<td>(iii) статьи, опубликованные до 2014 года  </td>
    </tr>
    <tr>
		<td>(iii) статьи с конференций </td>
    </tr>
    <tr>
		<td>(iv) научные статьи на неанглийском языке  </td>
    </tr>
    <tr>
		<td>(v) главы из книг </td>
    </tr>
    <tr>
		<td>(vi) статьи, не прошедшие рецензирование </td>
    </tr>
    <tr>
		<td>(vii) статьи со статистическим анализом </td>
    </tr>
</table>

</body>
</html>

#### 5.1. Стратегия поиска и критерии отбора
Поиск релевантных исследований по распознаванию эмоций начался с таких модальностей, как физиологические сигналы, включая ЭЭГ, ЭКГ, ГСР и ЭТ, речевые сигналы и изображения лиц. Кроме того, мы обратили внимание на ИИ, включая ML и DL. Мы провели поиск в известных электронных базах данных, включая Web of Science, MEDLINE, PubMed/PubMed Central, IEEE Explore, Scopus и Wiley, чтобы найти нужные статьи о распознавании эмоций. Авторы ограничили поиск статей английским языком и журнальными статьями. Окно поиска статей было настроено на последние 10 лет, охватывая статьи, опубликованные в период с декабря 2013 года по июль 2023 года. В фокусе поиска были физико-логические сигналы, распознавание или классификация эмоций, изображения лиц и искусственный интеллект. Ключевые термины, использованные для поиска соответствующих физиологических сигналов, включают "электроэнцефалограмма", "электрокардиограмма", "гальваническая реакция кожи", "слежение за глазами", "электроокулограмма", "ЭЭГ", "ГСР", "ЭОГ", "ЭТ", "ЭКГ", "ЭКГ" и "речь". Критерии поиска эмоций включают "классификацию эмоций", "обнаружение эмоций", "распознавание эмоций", "идентификацию эмоций" и "построение диаграмм эмоций". Поиск распознавания эмоций на основе изображений включает "изображения лиц", "изображения", "данные о лицах" и "лица". Наконец, для поиска искусственного интеллекта использовались такие слова, как "глубокое обучение", "машинное обучение", "автоматическое распознавание", "классификация" и "искусственный интеллект".
Поиск критериев для сканирования и отбора подходящих научных статей по распознаванию эмоций был утомительным и отнимал много времени. Поэтому мы приняли критерии включения и исключения для отбора релевантных статей для нашего обзорного исследования. В таблице 1 представлены принятые критерии включения и исключения научных статей. На первом этапе авторы проанализировали название, ключевые слова и аннотацию статей. После обсуждения авторы решили окончательно утвердить критерии включения/исключения статей. После первичного отбора полный текст оставшихся статей был изучен и проанализирован на втором этапе.
#### 5.2. Результаты
На рис. 8 показано руководство PRISMA, использованное для скрининга и отбора релевантных статей по распознаванию эмоций. Мы разделили отбор статей на три этапа: идентификация, отбор и включение. Первоначально было выявлено 14 257 статей из шести престижных баз данных и реестров, включая Web of Science, PubMed, MEDLINE, Inspec, Scopus и другие. Исходя из актуальности исследования, мы отобрали 3846 статей, а оставшиеся отбросили до отбора. На этапе отбора было получено 968 статей, из которых 234 были отобраны для дальнейшей оценки, а остальные исключены. На этапе окончательного включения из 234 статей 92 были исключены на основании критериев исключения, а 142 были отобраны для рассмотрения. Распределение включает 44 статьи, основанные на ЭЭГ, 20 статей о распознавании эмоций на основе ЭКГ, 16 статей о распознавании эмоций на основе GSR, 6 статей об ЭТ и по 28 статей о распознавании эмоций на основе речи и изображений лица. Однако некоторые статьи являются общими для распознавания эмоций на основе ЭЭГ, ЭКГ и GSR. На рис. 9 показано распределение статей по времени и издателям. Анализ по времени показывает, что наибольшее количество статей относится к 2022 году, а издательство Elsevier является наиболее предпочтительным, за ним следует IEEE.

![|800](https://i.imgur.com/ZPDhipv.png)

![|900](https://i.imgur.com/Ipq4SxL.png)

### 6. Обзор исследований по распознаванию эмоций с помощью сигналов ЭЭГ
Авторы отобрали 44 статьи, посвященные распознаванию эмоций на основе ЭЭГ. Кроме того, в 5 статьях сигналы ЭЭГ использовались наряду с модальностями ЭКГ и GSR, в результате чего общее количество статей составило 49. В таблице А.3 представлена подробная информация об автоматизированной системе распознавания эмоций на основе ЭЭГ.
#### 6.1. Основные моменты распознавания эмоций на основе ЭЭГ
Анализ по времени показывает, что наибольшее число исследований - 12 - было представлено в 2020 году, затем следуют 2021 и 2019 годы - по 9 исследований. Эликация эмоций с помощью аудио- и видеостимулов получила наибольшее распространение при получении ЭЭГ. Распознавание эмоций на основе ЭЭГ проводилось в основном на публичных наборах данных ЭЭГ, а не на частных. Чаще всего использовались DEAP, SEED/SSED IV и DREAMER - 22, 16 и 9 раз соответственно. Классификация эмоций на основе V/A/D предпочтительнее, чем дискретные эмоции и положительные (Pos)/отрицательные (Neg)/нейтральные (Neu). Классификация четырех базовых эмоций в дискретной модели является более предпочтительной по сравнению с другими дискретными моделями классификации. Извлечение нелинейных и статистических признаков предпочтительнее прямого извлечения признаков. Для распознавания эмоций на основе ЭЭГ наиболее часто используется метод спектральной плотности мощности (PSD). Для извлечения релевантных признаков чаще всего используются такие методы декомпозиции, как вейвлет-разложение, эмпирическое модовое разложение (EMD) и вариационное модовое разложение (VMD). Кроме того, для извлечения временно-частотного представления (ВЧП) использовались короткопериодное преобразование Фурье (STFT), класс Коэна и S-трансформация. Оценка с использованием k-кратной перекрестной валидации (FCV), особенно 10 FCV, была предпочтительнее, чем валидация с оставлением одного субъекта без внимания (LOSO) и валидация с удержанием. Для классификации эмоций ML-модели используются чаще, чем DL-модели. Подробная сводка распределения модели принятия решений представлена на рис. 10. Из рис. 10 видно, что больше всего используется SVM и его разновидность, за ним следуют KNN и классификатор машины экстремального обучения (ELM). В таксономии DL десять раз использовался CNN, за ним следуют модели принятия решений на основе LSTM.

![|600](https://i.imgur.com/OdPlhXm.png)

#### 6.2. Детали наборов данных эмоций на основе ЭЭГ
В таблице B.9 представлены подробные сведения о наборах данных ЭЭГ, использованных в исследованиях по распознаванию эмоций на основе ЭЭГ. Всего в исследованиях использовался 21 разнообразный набор данных ЭЭГ для распознавания эмоций. Среди них 15 общедоступных и 6 частных наборов данных. По одному исследованию использовали музыку, игры и VR в качестве возбудителя, 2 исследования использовали изображения, а остальные - аудиовизуальные стимулы.
### 7. Обзор исследований по распознаванию эмоций с помощью ЭКГ-сигналов
Всего было рассмотрено 23 статьи, посвященные распознаванию эмоций на основе ЭКГ, как показано в таблице А.4. Из 23 статей 20 относятся только к распознаванию эмоций на основе ЭКГ, а также к исследованиям на основе ЭЭГ и GSR.
#### 7.1. Основные аспекты распознавания эмоций на основе ЭКГ
Распределение распознавания эмоций на основе ЭКГ по годам показывает, что четыре статьи относятся к 2017, 2020 и 2021 годам, соответственно. На 2022 год приходится три статьи, по две статьи на 2019 и 2023 годы и по одной на 2014, 2015 и 2018 годы соответственно. Наиболее распространенным вариантом для вызова эмоций были аудио/видео и только видео, за ними следовали изображения и музыка на основе эмоций, которые вносили равный вклад в вызова эмоций. Для определения эмоционального состояния исследователи предпочли публичные наборы данных ЭКГ, а не частные. Из публичных наборов данных пять раз использовался набор DREAMER, по три раза - AMIGOS и ASCERTAIN, по два раза - WESAD и MAHNOB-HCI, и по одному разу - остальные. Наибольший результат показала классификация V/A/D, за ней следует классификация дискретных эмоций (четыре класса). Наибольшее предпочтение отдается извлечению признаков непосредственно из ЭКГ-сигналов. К ним относятся нелинейные признаки (NLF), статистические признаки (STSF), признаки во временной области (TDF), вариабельность сердечного ритма (HRV), признаки в частотной области (FDF) и ритмические признаки. Кроме того, для извлечения репрезентативных признаков использовались методы вейвлет-декомпозиции и EMD. Для валидации модели классификации в основном использовалась десятифакторная классификация (Ten-FCV), за которой следовали классификация с удержанием и LOSO. Распределение моделей принятия решений для классификации эмоций ЭКГ представлено на рис. 11. Как видно из рис. 11, для распознавания эмоций 15 раз использовались ML-модели и 8 раз - DL-модели. SVM и KNN наиболее эффективны для классификации ЭКГ в таксономии ML, в то время как CNN более распространена в таксономии DL.

![|600](https://i.imgur.com/rUuJeFI.png)

#### 7.2. Детали наборов данных эмоций на основе ЭКГ
Во всех статьях, включенных в наш обзор, было использовано в общей сложности 18 наборов данных эмоций на основе ЭКГ. Подробная информация о наборах данных эмоций на основе ЭКГ приведена в таблице B.10. В исследованиях по распознаванию эмоций использовались частные наборы данных, а не публичные наборы данных ЭКГ. Аудио/видео стимулы были наиболее предпочтительным выбором для вызова эмоций, за ними следовали музыка и стимулы на основе изображений. В системе сбора данных использовались три настройки электродов. Наибольшее распространение получил тип классификации эмоций V/A/D, за которым следует дискретная классификация эмоций.
### 8. Обзор исследований по распознаванию эмоций с помощью сигналов GSR
В общей сложности было рассмотрено 18 статей, посвященных распознаванию эмоций на основе GSR-сигналов, как показано в таблице А.5. Из 18 статей в 16 использовалось только распознавание эмоций на основе GSR-сигналов, а в 2 - в сочетании с исследованиями на основе ЭЭГ и ЭКГ.
#### 8.1. Основные аспекты распознавания эмоций на основе GSR
Анализ распознавания эмоций на основе GSR, включенных в обзор, по времени показывает, что наибольшее количество статей (4 статьи) относится к 2020 году. 2016 и 2017 годы включают по три исследовательские статьи, а 2018, 2019, 2021 и 2022 годы - по две статьи соответственно. В 2014, 2015 и 2023 годах статей нет. Чаще всего использовалась эпликация эмоций с помощью аудио/видео и музыкальных стимулов. Наборы данных DEAP и ASCERTAIN использовались по три раза, остальные - по одному разу. Исследователи использовали частные наборы данных GSR для распознавания эмоций (11 раз), а не публичные (9 раз). Классификация эмоций в терминах V/A и дискретных эмоций внесла одинаковый вклад. Для классификации использовалось прямое извлечение STSF, NLF, ритмических признаков и энтропийных признаков из GSR-сигналов. Кроме того, для извлечения информации из GSR использовались такие методы разложения, как вейвлет-разложение, DWT и EMD. Стратегия проверки также включает в себя удержание и k-кратное CV. Стратегии классификации, использованные для распознавания эмоций, показаны на рис. 12. Из рис. 12 видно, что ML-модели использовались чаще, чем DL-модели. Среди ML-моделей наиболее распространенной стратегией классификации была SVM и ее варианты (7 раз), за ними следуют KNN и ансамблевые методы (ET), использовавшиеся по два раза. Модели CNN, комбинация CNN с долговременной и кратковременной памятью (LSTM), были фаворитами в DL-моделях. Аудио/видео стимулы были наиболее предпочтительным выбором для вызова эмоций, за ними следовали музыкальные стимулы (см. рис. 12).

![|600](https://i.imgur.com/JffjnrF.png)

![|600](https://i.imgur.com/L69461N.png)

#### 8.2. Детали наборов данных эмоций на основе GSR
Во всех статьях, включенных в наш обзор, использовалось в общей сложности 14 наборов данных эмоций на основе GSR. Подробная информация о наборах данных эмоций на основе GSR приведена в таблице B.11. В исследованиях по распознаванию эмоций использовались частные наборы данных, а не публичные. Аудио/видео стимулы были наиболее предпочтительным выбором для вызова эмоций, за ними следовали музыка и стимулы на основе изображений. В системе сбора данных использовались три настройки электродов. Больше всего изучалась классификация эмоций на основе дискретных моделей эмоций, затем В/А/Д и состояния аффекта.
### 9. Резюме исследований по распознаванию эмоций с помощью сигналов ЭТ
Подробный обзор исследований распознавания эмоций на основе ЭТ представлен в таблице A.6. Всего было отобрано 6 статей, которые были включены в наш обзорный анализ.
#### 9.1. Основные аспекты распознавания эмоций на основе ЭТ
Распределение статей по годам показывает, что наибольшее количество статей - три - было опубликовано в 2021 году. Кроме того, в 2019, 2020 и 2023 годах было опубликовано по одной статье. В трех статьях использовалась элекция эмоций на основе видео, в двух - на основе изображений, и в одной - виртуальная реальность. В пяти статьях использовался частный набор данных эмоций ET, в то время как только один набор данных ET находится в открытом доступе. Во всех статьях исследовалась дискретная классификация эмоций, в четырех из них использовались четыре основные категории эмоций. Признаки STSF, FDF и NLP были извлечены непосредственно из сигналов ЭТ. В одной статье использовалось преобразование сигнала с помощью БПФ и STFT. Для проверки моделей чаще всего использовались валидация с удержанием и LOSO CV. Распределение моделей принятия решений для классификации показано на рис. 13. Из рис. 13 видно, что для классификации эмоций на основе ЭТ предпочтение отдается DL-моделям, а не ML-методам.
#### 9.2. Детали наборов данных эмоций на основе ЭТ
Подробные сведения о наборе данных эмоций на основе ET приведены в таблице B.12. Из этого обзора видно, что для распознавания эмоций использовались отдельные наборы данных. Кроме того, из шести наборов данных, используемых для распознавания эмоций, пять являются частными разработками, а один - общедоступным. Это ограничивает применимость и удобство использования распознавания эмоций на основе ЭТ. Эликация эмоций из видео использовалась три раза, из изображений - два раза, а виртуальная реальность - один раз.
### 10. Обзор исследований по распознаванию эмоций с помощью речевых сигналов
Для исследования распознавания эмоций на основе речевых сигналов мы отобрали 28 журнальных статей. Краткое содержание этих статей, использованных в обзорном анализе, представлено в таблице A.7.
#### 10.1. Основные аспекты распознавания эмоций на основе речевых сигналов
Как видно из сводной таблицы А.7, по одной статье было включено из 2014, 2015 и 2017 годов соответственно. Наибольшее количество статей, а именно 8, было представлено в 2019 году, затем 6 статей в 2020 году, 5 в 2021 году и по 3 в 2018 и 2022 годах, соответственно. Для вызова эмоций чаще всего использовались аудио/видео или аудиозаписи. Анализ наборов данных показывает, что наиболее предпочтительными для тестирования моделей оказались наборы данных EMO-DB, RAVDEES, CASIA и IEMOCAP. Наибольшая сила распознавания эмоций на основе речи заключается в том, что для проверки метода использовалось несколько наборов данных. Публичные наборы данных речевых эмоций были выбраны вместо частных. Во всех исследованиях использовалась классификация эмоций по дискретному типу. Для извлечения признаков чаще всего использовались спектральная плотность мощности (PSD), Мел-кепстральные коэффициенты (MFCC), спектрограмма Мела (MSG), STFT и варианты вейвлет-преобразования (WT). Валидация модели с помощью holdout CV была наиболее предпочтительна для речи, за ней следует k-FCV, а наименее - LOSO CV, соответственно. Сумма и распределение методов классификации, использованных для распознавания эмоций, представлены на рис. 14. Распределение, представленное на рис. 14, показывает, что DL-модели имеют преимущество перед ML-моделями для распознавания эмоций на основе речи. Классификатор SVM использовался 7 раз, а классификатор extreme learning machine (ELM) - 2 раза при принятии решений на основе ML. Для DL-моделей CNN использовался 10 раз, а LSTM и BiLSTM - по 3 раза.

![600](https://i.imgur.com/xYpb353.png)

#### 10.2. Детали наборов данных эмоций на основе речи
Подробный обзор наборов данных эмоций, основанных на речи, представлен в таблице B.13. Как видно из подробностей, в исследованиях по распознаванию эмоций на основе речи использовались 19 наборов данных. Из них 11 наборов данных находятся в открытом доступе, а 8 - в частном. Для классификации эмоций используются предпочитаемые речью дискретные модели эмоций с несколькими эмоциями, варьирующимися от 3 до 12.
### 11. Обзор исследований по распознаванию эмоций с использованием изображений лица
В обзор вошли 28 статей, посвященных распознаванию эмоций по изображениям лиц. В таблице A.8 представлен краткий обзор исследований распознавания эмоций по изображениям лица.
#### 11.1. Основные сведения о распознавании эмоций по изображениям лиц
Из таблицы A.8 видно, что наибольшее количество статей приходится на 2019 и 2020 годы соответственно. Распознавание эмоций по изображениям лиц содержит по одной статье за 2015, 2016 и 2017 годы соответственно. Из 2023, 2021 и 2022 годов было извлечено в общей сложности 2, 4 и 5 статей. Наборы данных CK+ и JAFFE являются наиболее часто используемыми наборами данных изображений лиц. Кроме того, во многих исследованиях использовались FER2013, RAF-DB и AffectNet. Исследования распознавания эмоций по изображениям лиц проверили свои модели на множестве наборов данных. Большинство наборов данных изображений лиц находятся в открытом доступе. Для классификации используется дискретная модель эмоций с несколькими эмоциями, варьирующимися от 2 до 10. Предпочтение отдается признакам, основанным на геометрических или текстурных характеристиках лица. Валидация модели с помощью стратегий holdout CV и k-FCV наиболее распространена. Распределение моделей принятия решений для изображений лиц показано на рис. 15. Из 28 статей в 20 статьях для классификации предпочитаются DL-модели, в 7 используются ML-модели, а статус одной статьи неизвестен. Для ML-моделей наиболее предпочтительным оказался классификатор SVM, в то время как CNN имеет преимущество перед другими DL-моделями.

![|600](https://i.imgur.com/ddtgcUB.png)

#### 11.2. Детали наборов данных эмоций на основе изображений лиц
Подробная информация о наборах данных изображений лиц, используемых для распознавания эмоций, приведена в таблице B.14. Всего в исследованиях, включенных в наш обзор, было использовано 24 набора данных, 21 набор находится в открытом доступе, и только 3 набора являются частными. Все наборы данных использовали дискретную классификацию эмоций.
### 12. Обсуждение
Как видно из таблиц А.3, А.4 и А.5, распознавание эмоций с помощью физиологических сигналов, таких как ЭЭГ, ЭКГ и ГСР, в основном классифицируется как валентность, возбуждение и доминантность. В случае сигналов ЭТ, речи и изображений предпочтение было отдано дискретной классификации эмоций, как показано в таблицах A.6, A.7 и A.8. Наиболее распространенной и предпочтительной техникой оказалась аудио/видео эликтрификация. В следующем подразделе представлено обсуждение отдельных модальностей для распознавания эмоций.
#### 12.1. Выводы из исследований по распознаванию эмоций на основе ЭЭГ
Сигналы ЭЭГ являются нелинейными и нестационарными с многочастотными компонентами [36-38]. Поэтому для извлечения значимой информации из многочастотных сигналов ЭЭГ предпочтение отдается методам декомпозиции [36-39]. Как видно из таблицы А.3, такие методы разложения, как дискретное вейвлет-преобразование (DWT), перестраиваемое Q-вейвлет-преобразование (TQWT), гибкое аналитическое вейвлет-преобразование (FAWT), комплексное вейвлет-преобразование с двойным деревом (DT-CWT), EMD, VMD и MVMD, широко использовались для извлечения нужных частотных полос и мгновенной информации о времени и частоте [40-55]. Признаки, извлеченные из подкомпонентов этих методов разложения, в дальнейшем используются для классификации с помощью методов ML. Кроме того, из-за высокого временного разрешения и наличия многочастотных компонентов преобразование временных рядов ЭЭГ в ТФР с помощью STFT, сглаженного распределения псевдо-Вигнера-Вилля (SPWVD), S-транс формы, распределения Вигнера-Вилля (WVD) и квадратичного распределения время-частота (QTFD) также было предпочтительным [56-64]. TFR, полученный с помощью этих методов, комбинируется с DL-моделями, такими как CNN, для распознавания эмоций. Анализ показывает, что самая высокая точность в 100 % была достигнута для классификации валентности, возбуждения и доминантности на наборе данных DREAMER [54]. Аналогично, точность 99,56 %, 99,67 % и 99,55 % для классификации возбуждения, доминирования и валентности была достигнута на наборе данных DEAP с использованием LOSO CV [54]. Нелинейные методы декомпозиции обеспечивают эффективное представление сигналов ЭЭГ, благодаря чему была достигнута самая высокая точность классификации [54]. Кроме того, извлечение ТФР из сигналов ЭЭГ с помощью SPWVD и TOR- на основе S-T в сочетании с CNN привело к точности 93,01% и 94,58% для классификации дискретных эмоций на частных наборах данных ЭЭГ [58,61]. Таким образом, из таблицы A.3 видно, что методы декомпозиции с ML-моделями и комбинация TFR с DL-моделями дали самые высокие результаты по точности распознавания эмоций.
#### 12.2. Выводы из исследований распознавания эмоций на основе ЭКГ
ЭКГ-сигналы являются квазистационарными с высоким отношением сигнал/шум (SNR) по сравнению с ЭЭГ-сигналами. Поэтому прямое извлечение признаков может помочь извлечь репрезентативную и значимую информацию из ЭКГ-сигналов. Так, в исследованиях, основанных на ЭКГ, предпочтение отдается прямому извлечению признаков в терминах NLF, STSF, ритмичности, TDF и FDF [53,65-75]. Поскольку ЭКГ является квазистационарной и содержит смешанные частотные компоненты, вейвлет-разложение и EMD-разложение также достигли высокой точности [65,76-78]. Методы ОД на основе SVM и KNN успешно классифицируют различные эмоции благодаря своей способности проводить точные границы между различными классами эмоций. Благодаря ритмичности и высокому SNR сигналов ЭКГ, методы DL извлекли репрезентативные признаки, что привело к высокой производительности системы [53,73,74,79-83]. Наибольшая точность в 100 % была достигнута для дискретной классификации эмоций с использованием ритмических признаков в сочетании с SVM-классификатором на частном наборе данных [66]. В другом исследовании ученые получили 100 % точность для классификации дискретных эмоций, а также для классификации валентности и возбуждения [77]. Авторы работы [77] использовали признаки на основе вейвлетов и вероятностную нейронную сеть (PNN) для классификации. Комбинация CNN и LSTM позволила получить точность 98,73 % и 90,5 % при использовании DL-моделей на публичных наборах данных AMIGOS и DREAMER [82].
#### 12.3. Выводы из исследований распознавания эмоций на основе GSR
Как и сигналы ЭЭГ и ЭКГ, сигналы GSR также являются нестационарными и нелинейными. Поэтому извлечение из них значимой и репрезентативной информации является предпочтительным. Характеристики извлекаются в виде NLF, STSF, энтропии, TDF, FDF и/или ритмов [53,67,84-90]. Были изучены методы декомпозиции на основе EMD и вейвлетов благодаря их способности извлекать важнейшие характеристики, необходимые для классификации эмоций [77,85,91-93]. Извлечение признаков или их декомпозиция облегчают классификаторам определение границ принятия решений для различных эмоций. Поэтому ML-модели, такие как SVM и KNN, обеспечивают очень высокую точность классификации. Кроме того, преобразование сигнала в другую область и применение DL-моделей было эффективно для распознавания эмоций [53,82,94,95]. Самая высокая точность в 100 % была получена для признаков, основанных на графиках Пуанкаре (PCP), экспоненте Ляпунова (LE) и приблизительной энтропии (APEN), с помощью классификатора PNN на наборе данных DEAP [87]. Аналогичное исследование, основанное на EMD и TDF с использованием SVM-классификатора, также позволило добиться идеальной классификации эмоций на частном наборе данных [91]. Кроме того, статистические признаки [89], вейвлет-анализ [77], NLF [90] и DWT [93] также достигли высокой точности при распознавании эмоций. Таким образом, прямое извлечение STSF, энтропии, TDF, FDF и NLF может обеспечить точную репрезентацию эмоций с помощью сигналов GSR. Кроме того, вейвлеты и методы декомпозиции могут извлекать дискриминационные характеристики из сигналов GSR для распознавания эмоций.
#### 12.4. Выводы из исследований распознавания эмоций на основе ЭТ
Из таблицы А.6 видно, что NLF, STSF и FDF сигналов ET могут обеспечить лучшее представление эмоций [96-99]. Кроме того, DL-модели могут извлекать репрезентативные признаки, что приводит к высокой точности [96,97,99]. Самая высокая точность в 92 % была достигнута с помощью STSF и глубокого многослойного перцептрона (DMLP) для классификации состояния валентности на наборе публичных данных eSEE-d. Однако для подтверждения этих результатов требуется дополнительный анализ. Кроме того, валидация модели с помощью holdout CV подвержена чрезмерной подгонке, поэтому может не дать такой же производительности при LOSO или k-FCV.
#### 12.5. Выводы из исследований распознавания эмоций на основе речи
Речевые сигналы имеют просодию, нестационарны, специфичны для языка и зависят от контекста. Кроме того, речевые сигналы нестационарны, а некоторые из них имеют периодичность [100]. Поэтому представление речи в виде спектральных признаков с использованием MFCC, MFC, STFT и WT оказалось эффективным для распознавания эмоций [101-112]. Ста- тистические признаки обеспечили дискриминантное представление речевых сигналов, благодаря чему была получена эффективная классификация эмоций [103,109,113,114]. Распознавание эмоций на основе речи достигло более высокой точности, когда CNN-модели были объединены со специальным представлением, включающим одновременную информацию о времени и частоте [107,109-112,114-117]. Как упоминалось ранее, из-за особенностей просодии и контекстной зависимости речи CNN, LSTM и BiLSTM, основанные на внимании, также остаются эффективными в классификации эмоций на основе речи [110,118-121]. Наибольшая точность в 100 % была достигнута на публичных наборах данных EMO-DB и CASIA при использовании признаков MFCC и классификатора линейного дискриминантного анализа [105].
#### 12.6. Выводы из исследований по распознаванию эмоций на основе изображений лица
Распознавание эмоций по изображениям лица предполагает использование характеристик лица. Поэтому наиболее эффективными оказались такие методы, как выделение лица, геометрические признаки, текстурные признаки и бинарные шаблоны [122-132]. Аналогично, поскольку эмоции распознаются с помощью изображений, модели CNN оказались наиболее эффективными моделями принятия решений благодаря их способности извлекать пространственно-временные характеристики. Модули внимания с CNN также доказали свою эффективность для определения геометрии лица при распознавании эмоций [129,133-136]. Самая высокая точность в 100 % была достигнута на наборе данных публичных изображений JAFFE с использованием конволюционных признаков и модели CNN [137]. Аналогично, точность 99,36 % была получена на наборе данных CK+ с использованием модели CNN [138]. Точность 99,59 % была достигнута на наборе данных MMI с использованием пространственно-временного признака оптического потока (OFSTF) в сочетании с моделью CNN [136].
#### 12.7. Общее резюме системы автоматического распознавания эмоций
Графическое представление автоматизированного распознавания эмоций для всех модальностей, использованных в данном обзоре, показано на рис. 16. Как видно из резюме, физиологические (ЭЭГ, ЭКГ, ЭТ и ГСР) и физические (речь) сигналы широко используются для извлечения признаков. Для извлечения значимой информации из сигналов ЭЭГ, ЭКГ и GSR чаще всего используется нелинейное разложение. Физиологические сигналы (ЭЭГ, ЭКГ, ГСР и ЭТ) содержат множество компонентов, которые имеют нестационарную и нелинейную природу. Поэтому такие методы разложения, как EMD, VMD и вейвлет-преобразование (DWT, TQWT, FAWT и другие), обеспечивают эффективное представление различных эмоциональных состояний. Кроме того, нелинейные и статистические признаки из мультикомпонентов ЭЭГ, ЭКГ, GSR и ЭТ дают наиболее представительные характеристики для распознавания эмоций. Широко используются частотно-доменные признаки для речи и прямое извлечение признаков для ЭТ. Глубинные признаки чаще всего используются для изображений лиц. Использование частотных коэффициентов Mel для речи и извлечение лица для изображений позволило получить дискриминативные признаки для распознавания эмоций. Наконец, для принятия решений наиболее эффективным и предпочтительным классификатором для сигналов ЭЭГ, ЭКГ, GSR и ЭТ оказался модальный ML на основе SVM. Обзорные исследования показывают, что для речевых сигналов и изображений лиц принятие решений с использованием DL-моделей на основе CNN может привести к наибольшей эффективности. Модели CNN имеют встроенные конволюционные слои, которые уменьшают высокую размерность изображений без потери информации. Поэтому CNN-модели могут эффективно извлекать признаки из изображений и обучаться распознаванию паттернов, что делает их хорошо подходящими для распознавания эмоций. Кроме того, методы извлечения и преобразования признаков широко используются для входных сигналов временных рядов, включая ЭЭГ, ЭКГ, GSR, речь и ЭТ. Общий анализ показал, что объединение информации помогает улучшить производительность системы. Исследование показывает, что слияние ЭЭГ с ЭКГ/ГСР, ЭКГ с ГСР или слияние различных признаков обеспечивает более высокую точность, чем при использовании одной модальности [71,82,84,88,92,93]. Таким образом, объединение признаков и сенсоров, полученных из нескольких источников, может стать лучшим вариантом для распознавания эмоций.
Общая сводка модальностей, рассмотренных в нашем обзоре для распознавания эмоций, с указанием их сильных и слабых сторон/будущих рекомендаций приведена в таблице 2. Следует отметить, что эта сводка составлена на основе наших наблюдений за работами, включенными в систематический обзор.

![|1100](https://i.imgur.com/hViZzDo.png)

### 13. Проблемы
После тщательного изучения автоматизированных систем распознавания эмоций мы выявили потенциальные проблемы в существующих исследованиях. Ниже перечислены основные проблемы автоматизированных систем распознавания эмоций:
#### 13.1. Наборы данных
Большинство наборов данных, используемых для распознавания эмоций, находятся в открытом доступе. Однако большинство из них было использовано по максимуму, что позволило добиться наивысшей точности классификации. Кроме того, доступные наборы данных были получены с помощью одной модальности, т. е. либо для ЭЭГ, ЭКГ, ЭТ, ГСР, речи или изображений лица. Поэтому до сих пор существует пробел в исследованиях по анализу распознавания эмоций с использованием нескольких модальностей у одного и того же субъекта. Кроме того, отсутствие общедоступных наборов данных по эмоциям для здравоохранения, интерфейсов мозг-компьютер и других приложений ограничивает возможности такого анализа.
#### 13.2. Адаптивный анализ и классификация
Физиологические и физические сигналы нелинейны, имеют множество частотных составляющих и изменяются спонтанно [39,139,140]. Точный и эффективный анализ таких сигналов может быть выполнен с помощью методов выделения и декомпозиции признаков. Однако для извлечения значимой информации из такого сигнала требуется настройка параметров [46,47]. Однако, как показывает наш обзор, адаптивный анализ таких сигналов изучался мало. Эти модели, управляемые данными, были протестированы на частных наборах данных ЭЭГ [46,47]. Таким образом, адаптивный анализ может быть использован для извлечения репрезентативной информации из ЭЭГ, ЭКГ, ЭТ, GSR и/или речевых сигналов. Аналогично, для классификации модели ML и/или DL требуют тщательной настройки гиперпараметров для достижения оптимальной производительности. Эмпирические и фиксированные настройки гиперпараметров могут не дать желаемого результата.
#### 13.3. Отсутствие обобщения
Сбор физиологических и физических сигналов осуществлялся с помощью различных систем. Различные характеристики систем и время получения сигналов приводят к созданию последовательностей различной длины. Наш обзорный анализ показывает, что исследования по распознаванию эмоций с использованием ЭЭГ, ЭКГ, GSR и речевых сигналов анализировались с различной длиной сегментов. Изменение длительности анализируемых сигналов может не дать желаемого результата. Отсутствие информации и обобщений по выбору длины сигнала затрудняет доверие к решению, принимаемому разработанными моделями.
#### 13.4. Недоверие к автоматизированному принятию решений
Трудно доверять результатам работы такой автоматизированной системы, особенно когда полученные результаты противоречат или конфликтуют с предыдущими знаниями или ожиданиями. В результате заинтересованные стороны, специалисты и врачи не решаются полагаться на существующие модели при принятии решений. Именно поэтому, несмотря на ряд значительных технологических усовершенствований в области обработки сигналов, проектирования функций и искусственного интеллекта, эти модели не могут завоевать доверие экспертов. Кроме того, в исследовательских учреждениях редко используются системы поддержки принятия решений в режиме реального времени. Это связано с неспособностью существующих методов идентификации эмоций объяснить прогнозы, предоставляемые системами поддержки принятия решений. Чтобы вызвать доверие к автоматизированным системам, модели должны объяснять экспертам суждения, выносимые автоматизированной системой.
### 14. Будущие рекомендации и направления исследований
Проведенный нами обзор выявил нерешенные исследовательские проблемы в существующих системах распознавания эмоций. Будущие исследования должны быть сосредоточены на инновационных способах улучшения нашего понимания многочисленных модальностей и приложений. Ниже описаны возможные направления будущих исследований.

<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
    <h2>Обзор исследований по распознаванию эмоций</h2>
    <table>
        <tr>
            <th>Модальность</th>
            <th>Преимущества</th>
            <th>Рекомендации по развитию</th>
        </tr>
        <tr>
            <td>ЭЭГ</td>
            <td>
                <ul>
                    <li>Хорошо изучено</li>
                    <li>Комплексный анализ признаков</li>
                    <li>Исследованы модели ML и DL</li>
                    <li>Достигнута максимальная точность</li>
                    <li>Проверка на нескольких наборах данных</li>
                    <li>Доступные общедоступные наборы данных</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Несостоятельность в производительности</li>
                    <li>Исчерпывающее использование доступных наборов данных</li>
                    <li>Тестирование на очищенных и предварительно обработанных данных</li>
                    <li>Отсутствие адаптивности</li>
                    <li>Отсутствие объяснимости</li>
                    <li>Неравномерность выбора длины сегмента ЭЭГ</li>
                    <li>Ограниченное использование настройки гиперпараметров</li>
                    <li>Ограниченное использование техник слияния</li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>ЭКГ</td>
            <td>
                <ul>
                    <li>Хорошо изучено</li>
                    <li>Достигнута максимальная точность</li>
                    <li>Проверка на нескольких наборах данных</li>
                    <li>Доступные общедоступные наборы данных</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Несостоятельность в производительности</li>
                    <li>Исчерпывающее использование доступных наборов данных</li>
                    <li>Тестирование на очищенных и предварительно обработанных данных</li>
                    <li>Отсутствие адаптивности</li>
                    <li>Исследованы в основном модели ML</li>
                    <li>Отсутствие объяснимости</li>
                    <li>Неравномерность выбора длины сегмента ЭКГ</li>
                    <li>Ограниченное использование настройки гиперпараметров</li>
                    <li>Ограниченное использование техник слияния</li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Гальваническая кожная реакция (ГКР)</td>
            <td>
                <ul>
                    <li>Хорошо изучено</li>
                    <li>Достигнута максимальная точность</li>
                    <li>Проверка на нескольких наборах данных</li>
                    <li>Доступные общедоступные наборы данных</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Несостоятельность в производительности</li>
                    <li>Исчерпывающее использование доступных наборов данных</li>
                    <li>Тестирование на очищенных и предварительно обработанных данных</li>
                    <li>Отсутствие адаптивности</li>
                    <li>Исследованы в основном модели ML</li>
                    <li>Отсутствие объяснимости</li>
                    <li>Неравномерность выбора длины сегмента ГКР</li>
                    <li>Ограниченное использование настройки гиперпараметров</li>
                    <li>Ограниченное использование техник слияния</li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Eye Tracking (ET)</td>
            <td>
                <ul>
                    <li>Использование наборов данных, сгенерированных из различных стимулов</li>
                    <li>Прямое извлечение признаков</li>
                    <li>Создание простых моделей</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Несостоятельность в производительности</li>
                    <li>Ограниченное количество общедоступных наборов данных</li>
                    <li>Отсутствие адаптивности</li>
                    <li>Отсутствие объяснимости</li>
                    <li>Ограниченное использование настройки гиперпараметров</li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Речь</td>
            <td>
                <ul>
                    <li>Комплексный анализ методов извлечения признаков</li>
                    <li>Модели созданы и проверены на нескольких наборах данных</li>
                    <li>Доступность общедоступных наборов данных</li>
                    <li>Использование методов машинного обучения и глубокого обучения</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Модели не основаны на данных</li>
                    <li>Акцент на признаках частотной области</li>
                    <li>Несостоятельность в производительности</li>
                    <li>Отсутствие адаптивности</li>
                    <li>Отсутствие объяснимости</li>
                    <li>Ограниченное использование настройки гиперпараметров</li>
                </ul>
            </td>
        </tr>
        <tr>
            <td>Изображения лица</td>
            <td>
                <ul>
                    <li>Модели созданы и проверены на нескольких наборах данных</li>
                    <li>Доступность общедоступных наборов данных</li>
                    <li>Использование методов машинного обучения и глубокого обучения</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Модели не основаны на данных</li>
                    <li>Несостоятельность в производительности</li>
                    <li>Отсутствие адаптивности</li>
                    <li>Отсутствие объяснимости</li>
                    <li>Ограниченное использование настройки гиперпараметров</li>
                </ul>
            </td>
        </tr>    
    </table>
</body>
</html>



#### 14.1. Применение распознавания человеческих эмоций
Распознавание эмоций имеет множество применений, включая интерфейсы мозг-компьютер, робототехнику и здравоохранение. Однако с недавним технологическим прогрессом и ростом использования электронных гаджетов распознавание эмоций может помочь ускорить процесс в различных областях. Некоторые из них перечислены ниже:
##### 14.1.1. Обнаружение и мониторинг медицинских состояний
Эмоции человека могут содержать важную информацию о состоянии здоровья и многочисленных расстройствах. Были проведены исследования вариаций эмоций при болезни Паркинсона (БП), шизофрении, болезни Альцгеймера (БГА), синдроме дефицита внимания и гиперактивности (СДВГ), расстройствах аутистического спектра (РАС), эпилепсии и депрессии. При БП наблюдаются изменения эмоциональных состояний. Изменения эмоциональных состояний при БП наблюдались с помощью мимики, речи и сигналов ЭЭГ [141-144]. Немногочисленные исследования были проведены и в отношении вариаций эмоций при шизофрении. Исследования показали, что мимика, слуховые и ЭЭГ-сигналы измеряют эмоциональные состояния при шизофрении [145-147]. Тест "Чтение разума по глазам", выражение лица, моргание глаз и контекстуальные особенности показывают вариации эмоций при ЗПР [148-151]. Выражение лица, текст, сигналы ЭЭГ и исследования, основанные на эмодзи, показали эмоциональные изменения при депрессии [152-154]. Изучались изменения эмоциональных состояний при СДВГ по мимике и социальному познанию [155-157]. Изучение эмоций по мимике, видеоиграм, речевым сигналам и ЭЭГ использовалось для выявления аутизма [158-161]. Аналогичным образом мимика и социальное познание могут быть обнаружены при судорогах и эпилепсии [162,163]. Таким образом, можно провести тщательное исследование для выявления различных расстройств по эмоциям. Однако из-за отсутствия общедоступных наборов данных исследований очень мало. На рис. 17 показана автоматизированная система обнаружения физиологических и неврологических расстройств на основе эмоций.

![|1100](https://i.imgur.com/JzV3gnn.png)

##### 14.1.2. Здоровье детей
Изучение и анализ эмоций у детей также может сыграть важную роль в мониторинге их здоровья. Исследования показали, что эмоциональное развитие и регуляция могут иметь решающее значение для детей с дислексией [164-166], депрессией [167,168], тревогой [169,170] и аутизмом [171- 173]. Поэтому изучение мимики, речи и физио-логических сигналов может быть использовано для выявления аутизма, депрессии, тревожности и дислексии. Кроме того, распознавание эмоций может сыграть важную роль в обучении детей с аутизмом и дислексией.
##### 14.1.3. Исследования здоровья окружающей среды
Еще одно потенциальное применение распознавания человеческих эмоций - исследования здоровья окружающей среды. Известно, что физическая среда может оказывать влияние на эмоции и, в конечном счете, влиять на психическое здоровье. Например, экологические стрессоры (например, загрязнение воздуха и шум) могут быть связаны с рядом негативных эмоций, таких как недовольство, гнев, разочарование, неудовлетворенность, беспомощность, тревога и возбуждение [174,175]. Однако глубокое понимание психических эффектов, вызванных различными факторами окружающей среды, ограничено, в частности, сложностью измерения сложных эмоциональных состояний у людей.
##### 14.1.4. Взаимодействие человека и робота
Развитие искусственного интеллекта стимулировало разработку машин, моделирующих человека. Применение человеческих эмоций привлекло внимание исследователей к изучению человеко-машинных интерфейсов и анализу чувств. Человеко-машинные интерфейсы могут угадывать и понимать человеческие эмоции, что делает их более успешными во взаимодействии с людьми; модели должны уметь интерпретировать человеческие эмоции и адаптировать свое поведение соответствующим образом, что приведет к приемлемой реакции на эти чувства.
##### 14.1.5. Помощь пациентам
Эмоции могут играть ключевую роль в наблюдении за пациентом и оказании ему помощи. Эффективный анализ эмоций может помочь почувствовать и обнаружить одиночество, колебания настроения и суицидальные сигналы.
##### 14.1.6. Помощь при вождении
Распознавание эмоций также может быть использовано для определения усталости водителя. Выражение лица, движения глаз и/или ЭЭГ могут использоваться для мониторинга усталости водителя в режиме реального времени.
##### 14.1.7. Образование
Точный и эффективный анализ эмоций может помочь в изучении уровня удовлетворенности студентов образованием.
##### 14.1.8. Маркетинг
Камеры с системами искусственного интеллекта в торговых центрах могут использоваться для считывания эмоций покупателей в режиме реального времени, что может быть использовано в маркетинге.
##### 14.1.9. Рекрутинг
Автоматизированный анализ системы автоматического распознавания эмоций может быть использован для подбора персонала. Анализ эмоций во время собеседований может использоваться для мониторинга уровня стресса кандидатов.
##### 14.1.10. Бизнес-модели
Люди демонстрируют множество выражений и мыслей по поводу различных товаров. Ритейлеры могут использовать мысли и чувства покупателей для улучшения качества обслуживания в магазине. Цель исследования - сравнить данные типичных оценок удовлетворенности с данными технологий распознавания эмоций, чтобы определить, может ли распознавание эмоций дать полную картину или, возможно, заменить измерения удовлетворенности [176,177].
##### 14.1.11. Электронное обучение
Со времен COVID мы наблюдаем резкий рост использования электронных гаджетов и интернет-услуг. Онлайн-среды и виртуальные классы могут обеспечить непрерывное обучение, а технология обнаружения эмоций помогает определить уровень эмоциональности и понимания учащихся в режиме реального времени. Эта информация может быть использована для создания контента класса с учетом различных способностей детей к обучению [178,179].
#### 14.2. Генерация мультимодальных публичных наборов данных
Эмоции человека можно изучать для выявления различных расстройств, однако такие исследования не получили максимального распространения. Одна из причин - отсутствие доступных и разнообразных наборов данных. Поэтому разработка таких наборов данных и предоставление их в свободный доступ исследовательскому сообществу может способствовать выявлению физиологических расстройств на основе эмоций. Кроме того, вместо того чтобы сосредоточиться на одномодальном наборе данных, разработка мультимодального набора данных может обогатить и раскрыть более широкие возможности для расширенных исследований распознавания эмоций. Критерии доступа и авторизации должны быть простыми и быстрыми, чтобы специалисты могли избежать длительного ожидания. Методики и процессы сбора данных должны быть доступны, чтобы другие исследовательские организации могли повторить их и собрать больше данных для изучения.
#### 14.3. Разработка носимых систем распознавания эмоций
Физические сигналы, включая речь, жесты, выражение лица, текст, позу и т. д., подвержены ложным срабатываниям. Такие сигналы могут быть добровольно изменены, что приводит к ложным классификациям эмоций [47,180]. Наш обзорный анализ показывает, что сигналы ЭЭГ широко используются для распознавания эмоций, однако использование многочисленных датчиков ЭЭГ для их получения усложняет систему. Эмоции также распознаются по сигналам ЭКГ, в которых используются только три канала [65,67,81, 181]. Таким образом, использование ЭКГ-сигналов для распознавания эмоций выгодно с точки зрения количества датчиков и высокого соотношения сигнал/шум [39]. Центральная нервная система человека построена таким образом, что изменения в одном органе влияют на другой. В результате связь между мозгом и сердцем, взаимодействие мозга и глаз, а также связь мозг-сердце-глаза-мышцы может быть критически важной и полезной для анализа изменений во многих органах [39,182]. Сигналы фотоплетизмографии (PPG) обеспечивают лучшее представление взаимодействия мозга и сердца [183,184]. Преимущество PPG в том, что для сбора сигнала не требуется специальных установок или большого количества электродов. Датчики крепятся к наручным часам, пальцам или другим носимым устройствам, которые более доступны, менее дороги и более практичны, чем другие физиологические сигналы.
#### 14.4. Распределенные модели обучения
Огромные объемы данных производятся благодаря развитию ИИ и технологии больших данных. ИИ переживает бум благодаря извлечению ценной информации из большого объема данных и обладает потенциалом для развития человеческого общества. Современные модели автоматического распознавания эмоций разрабатываются с помощью централизованного ИИ. Однако данные, полученные из разных регионов, могут иметь субъективные изменения, географические различия и инструментальные различия, что может привести к динамическим изменениям в работе традиционных моделей ML. Кроме того, традиционная ML использует централизованную модель обучения, которая страдает от проблем конфиденциальности и коммуникационной нагрузки. Для преодоления этой проблемы может быть использовано федеративное обучение (FL). Основная цель FL - перенести обучение модели с центрального сервера на клиентские устройства, позволяя множеству клиентских наборов данных совместно работать над обучением модели, защищая конфиденциальность данных и снижая затраты на связь. При этом используется режим обучения "данные неподвижны, модель перемещается", в отличие от метода централизованного обучения "модель неподвижна, данные перемещаются" [185,186].
#### 14.5. Слияние информации
Слияние информации, также известное как слияние данных, - это процесс, который объединяет, интегрирует и анализирует данные из многочисленных источников для получения более детального и точного представления о желаемом явлении. Основная цель слияния информации - извлечение тонкой информации из различных, часто несовершенных источников данных, что приводит к улучшению процесса принятия решений, повышению уровня понимания и производительности в различных областях применения. Существуют различные уровни и типы слияния информации, как показано на рис. 18 и рассмотрено ниже:

![|800](https://i.imgur.com/vPrHUAv.png)

##### 14.5.1. Слияние на уровне датчиков
Этот уровень включает в себя интеграцию необработанных данных от каждого датчика без какой-либо обработки или аналитики. Он используется для повышения качества данных, уменьшения шума, а также для решения проблемы недостающих или неверных данных с некоторых датчиков [187].
##### 14.5.2. Слияние на уровне признаков
На этом уровне объединяются данные из нескольких источников, которые были предварительно обработаны и извлечены важные характеристики перед объединением [188]. Этот метод направлен на минимизацию размерности данных и создание единого представления признаков для последующего анализа.
##### 14.5.3. Слияние моделей
Слияние моделей или техника ансамбля моделей повышает производительность и обобщенность прогностических моделей [189]. Слияние моделей основано на понятии объединения предсказаний различных независимых моделей таким образом, чтобы получить окончательное, более надежное предсказание. Объединение возможностей различных моделей часто приводит к повышению общей точности прогноза и снижению риска чрезмерной подгонки.
##### 14.5.4. Слияние на уровне данных
Объединение данных на уровне данных включает в себя как объединение на уровне датчиков, так и объединение на уровне признаков [190]. Оно подразумевает объединение необработанных данных от множества датчиков и последующее извлечение важных характеристик из объединенных данных.
##### 14.5.5. Гибридное слияние
Гибридный уровень объединяет два или более уровня методов слияния, например, слияние на уровне признаков и на уровне решений. Предположим, классификация физических сигналов выполняется с помощью слияния на уровне признаков с одним классификатором, принимающим решения. С другой стороны, анализ физико-логических сигналов может быть выполнен с помощью слияния на уровне решений. Итоговое решение генерируется путем интеграции слияний на уровне признаков и на уровне решений для получения желаемой эффективности [191,192].
#### 14.6. Применение объяснимости
Объяснимый искусственный интеллект (XAI) - это стратегия разработки систем ИИ, которая пытается дать явные и понятные объяснения решениям модели ИИ. Принятие решений в моделях ИИ, таких как SVM, может быть сложным для понимания. Отсутствие прозрачности создает проблемы, особенно в таких важных приложениях, как здравоохранение, где знание логики, лежащей в основе решений ИИ, имеет решающее значение для доверия, точности и безопасности. Эти проблемы решаются с помощью подходов XAI, которые делают модели ИИ более наглядными и интерпретируемыми. Клиенты, специалисты и заинтересованные стороны могут понять, как система ИИ пришла к определенному результату, предоставляя человекочитаемые объяснения. Объяснения, предоставляемые подходами XAI, прозрачны, что очень важно для определения доверия к модели, ее предвзятости и справедливости, отладки и улучшения. Для ML-моделей используются такие методы, как визуализация признаков (изучение закономерностей в данных), модели на основе правил (явные правила принятия решений), локальные объяснения (локальные объяснения направлены на объяснение конкретных предсказаний или решений) и важность признаков (LIME (Local Interpretable Model-agnostic Explanations) и SHAP (SHapley Addi- tive ExPlanations)) [193]. Для CNN в качестве объяснений используются тепловые карты (class activation map (CAM)), включая Grad-CAM, Grad-CAM++, SMOOTHGRAD, U-CAM, Eigen-CAM и Score-CAM [184]. Обзор традиционных ML-моделей и XAI-моделей представлен на рис. 19.

![|900](https://i.imgur.com/b824pHR.png)

#### 14.7. Квантификация неопределенности
Квантификация неопределенности (КН) - это набор математических и вычислительных инструментов для оценки и характеристики неопределенности в вычислительных моделях, симуляциях и анализе данных [194-196]. Понимание неопределенности, связанной с результатами, является критически важным во многих научных и технических областях, поскольку от этого зависит точность прогнозов [194]. Неопределенность может возникать из различных источников, включая формулировку модели (в результате упрощений, предположений или приближений), входные данные (шум, недостающие данные или ошибки измерений), параметры модели (фиксированные параметры), приближения, начальные и граничные условия [197]. Источники неопределенности могут быть измерены с помощью UQ, который направлен на решение следующих вопросов:
- Как неопределенности во входных параметрах влияют на прогнозы модели?
- Каковы источники неопределенности в модели и ее входных параметрах?
- Насколько надежны прогнозы модели?
- Как мы можем улучшить модель и уменьшить неопределенность?
UQ предполагает оценку распределений вероятностей, статистических моментов (среднего, дисперсии и т. д.) и доверительных интервалов, которые указывают на неопределенность, связанную с результатами. Некоторые известные методы, используемые для оценки неопределенности, включают байесовский вывод, методы, основанные на дисперсии, методы Монте-Карло, вероятностную коллокацию, ансамблевое моделирование и бутстреппинг [194,198]. Графический обзор количественной оценки неопределенности детерминированной модели представлен на рис. 20.

![|1000](https://i.imgur.com/8baqEtq.png)

### 15. Заключение
Распознавание эмоций играет важную роль во многих областях, включая здравоохранение, электронное обучение, онлайн-покупки и т. д. В нашей статье представлен тонкий анализ человеческих эмоций. Всесторонний анализ систем распознавания эмоций показывает, что методы декомпозиции обеспечивают понимание информации, которая извлекает репрезентативные характеристики из физиологических сигналов. Принятие решений на основе SVM ML оказалось
оказалась наиболее эффективной и предпочтительной моделью распознавания эмоций. Способность DL-моделей автоматически извлекать и классифицировать глубокие признаки набирает популярность, и в последнее время все чаще используются CNN-модели. Наш обзорный анализ показывает, что объединение признаков и данных помогает улучшить общую производительность системы. Следовательно, в будущих моделях распознавания эмоций необходимо использовать слияние информации. Эмоции могут быть очень полезны в некоторых областях здравоохранения, таких как выявление болезни Альцгеймера, болезни Паркинсона, депрессии и шизофрении, а также в электронном обучении, анализе рынка и взаимодействии человека и робота. Однако в этих областях исследования систем распознавания эмоций человека ограничены из-за отсутствия общедоступных наборов данных. Поэтому в нашем обзоре рекомендуется разрабатывать и предоставлять доступные публичные наборы данных для расширения сферы применения исследований человеческих эмоций. Обзор показывает, что модели глубокого обучения завоевали популярность по сравнению с традиционными ML. Поэтому для распознавания эмоций можно использовать комбинацию гибридных методов глубокого обучения с использованием CNN, автоэнкодеров, LSTM и переходных моделей. Кроме того, точные универсальные модели могут быть разработаны с помощью объединенного метаобучения для обучения автоматизированных систем на различных наборах данных для конкретного приложения. Наконец, мы подчеркиваем важность объяснимости модели и количественной оценки неопределенности в распознавании эмоций для повышения доверия и общего влияния моделей ИИ.

![|1300](https://i.imgur.com/wgVizIP.png)
![|1300](https://i.imgur.com/na1JwoG.png)
![|1300](https://i.imgur.com/JSWesIh.png)
![|1300](https://i.imgur.com/eDZ6tsU.png)
![|1300](https://i.imgur.com/f1fp3J9.png)
![|1300](https://i.imgur.com/YsbZmFV.png)
![|1300](https://i.imgur.com/xJ1vLls.png)

### Заявление об авторском вкладе CRediT
Смит К. Кхаре: Концептуализация, методология, написание - первоначальный проект, проверка, редактирование. Виктория Бланес-Видал: Проверка, повторный просмотр и редактирование. Эсмаил С. Надими: проверка, рецензирование и редактирование. У. Раджендра Ачарья: Концептуализация, валидация, рецензирование и редактирование.
#### Декларация о конкурирующих интересах
Авторы заявляют, что у них нет известных конкурирующих финансовых интересов или личных отношений, которые могли бы повлиять на работу, представленную в данной статье.
#### Доступность данных
Для исследования, описанного в статье, данные не использовались
#### Приложение A. Краткое описание исследований распознавания эмоций
См. таблицы A.3-A.8.
#### Приложение B. Краткое описание наборов данных по эмоциям
См. таблицы B.9-B.14. 
### Приложение C. Сокращения
См. таблицу C.15.
![|1300](https://i.imgur.com/Ufw3oz4.png)
![|1300](https://i.imgur.com/big6CO8.png)
![|1300](https://i.imgur.com/UReAWsF.png)
![|1300](https://i.imgur.com/P9jeuLy.png)
![|1300](https://i.imgur.com/KrXOfJt.png)
![|1300](https://i.imgur.com/K3Cf8BR.png)
![|1300](https://i.imgur.com/69HRjK0.png)
![|1300](https://i.imgur.com/NyhHf3S.png)
![|1300](https://i.imgur.com/El4IFPC.png)
![|1300](https://i.imgur.com/FUBM5cj.png)
![|1300](https://i.imgur.com/mfixgXE.png)
![|1300](https://i.imgur.com/ByuqS50.png)
![|1300](https://i.imgur.com/e1bAUGE.png)
![|1300](https://i.imgur.com/rPNyVZM.png)
![|1300](https://i.imgur.com/zfxVLPm.png)
![|1300](https://i.imgur.com/KjCB1wh.png)
![|1300](https://i.imgur.com/YtHNWTh.png)
![|1300](https://i.imgur.com/0UB9zFH.png)
![|1300](https://i.imgur.com/4GF9XSu.png)
![|1300](https://i.imgur.com/YBNADf3.png)
![|1300](https://i.imgur.com/Y221srK.png)

### References
[1] K. Kamble, J. Sengupta, A comprehensive survey on emotion recognition based on electroencephalograph (EEG) signals, Multimedia Tools Appl. (2023) 1–36.
[2] R.E. Dahl, A.G. Harvey, Sleep in children and adolescents with behavioral and emotional disorders, Sleep Med. Clin. 2 (3) (2007) 501–511, http://dx. doi.org/10.1016/j.jsmc.2007.05.002, Sleep in Children and Adolescents. URL https://www.sciencedirect.com/science/article/pii/S1556407X07000513.
Information Fusion 102 (2024) 102019
[3] T.E. Feinberg, A. Rifkin, C. Schaffer, E. Walker, Facial discrimination and emotional recognition in schizophrenia and affective disorders, Arch. Gen. Psychiatry 43 (3) (1986) 276–279, http://dx.doi.org/10.1001/archpsyc.1986. 01800030094010.
[4] I.B. Mauss, A.S. Troy, M.K. LeBourgeois, Poorer sleep quality is associated with lower emotion-regulation ability in a laboratory paradigm, Cogn. Emot. 27 (3) (2013) 567–576, http://dx.doi.org/10.1080/02699931.2012.727783, PMID: 23025547. arXiv:https://doi.org/10.1080/02699931.2012.727783.
[5] M.N. Dar, M.U. Akram, R. Yuvaraj, S. Gul Khawaja, M. Murugappan, EEG-based emotion charting for Parkinson’s disease patients using Convolutional Recurrent Neural Networks and cross dataset learning, Comput. Biol. Med. 144 (2022) 105327, http://dx.doi.org/10.1016/j.compbiomed.2022.105327, URL https:// www.sciencedirect.com/science/article/pii/S0010482522001196.
[6] J. Sun, J. Han, Y. Wang, P. Liu, Memristor-based neural network circuit of emotion congruent memory with mental fatigue and emotion inhibition, IEEE Trans. Biomed. Circuits Syst. 15 (3) (2021) 606–616, http://dx.doi.org/10. 1109/TBCAS.2021.3090786.
[7] S.S. Jasim, A.K.A. Hassan, Modern drowsiness detection techniques: A review, Int. J. Electr. Comput. Eng. 12 (3) (2022) 2986.
[8] P. Lucey, J.F. Cohn, I. Matthews, S. Lucey, S. Sridharan, J. Howlett, K.M. Prkachin, Automatically detecting pain in video through facial action units, IEEE Trans. Syst. Man Cybern. B 41 (3) (2011) 664–674, http://dx.doi.org/10. 1109/TSMCB.2010.2082525.
[9] N. Jamil, N.H.M. Khir, M. Ismail, F.H.A. Razak, Gait-based emotion detection of children with autism spectrum disorders: a preliminary investigation, Procedia Comput. Sci. 76 (2015) 342–348.
[10] S. López-Martín, J. Albert, A. Fernández-Jaén, L. Carretié, Emotional distraction in boys with ADHD: Neural and behavioral correlates, Brain Cogn. 83 (1) (2013) 10–20, http://dx.doi.org/10.1016/j.bandc.2013.06.004, URL https:// www.sciencedirect.com/science/article/pii/S0278262613000845.
[11] T. Kircher, V. Arolt, A. Jansen, M. Pyka, I. Reinhardt, T. Kellermann, C. Konrad, U. Lueken, A.T. Gloster, A.L. Gerlach, A. Ströhle, A. Wittmann, B. Pfleiderer, H.-U. Wittchen, B. Straube, Effect of cognitive-behavioral therapy on neural correlates of fear conditioning in panic disorder, Biol. Psychiat. 73 (1) (2013) 93–101, http://dx.doi.org/10.1016/j.biopsych.2012.07.026, Struc- tural and Functional Activity with Stress and Anxiety. URL https://www. sciencedirect.com/science/article/pii/S0006322312006701.
[12] T. Dalgleish, The emotional brain, Nat. Rev. Neurosci. 5 (7) (2004) 583–589.
[13] T.S. Rached, A. Perkusich, Emotion recognition based on brain-computer interface systems, in: R. Fazel-Rezai (Ed.), Brain-Computer Interface Systems,
IntechOpen, Rijeka, 2013, http://dx.doi.org/10.5772/56227, Ch. 13.
[14] P. Ekman, An argument for basic emotions, Cogn. Emot. 6 (3–4) (1992)
169–200.
[15] R. Plutchik, H. Kellerman, Theories of Emotion, Vol. 1, Academic Press, 2013.
[16] G.F. Wilson, C.A. Russell, Real-time assessment of mental workload using
psychophysiological measures and artificial neural networks, Hum. Factors 45
(4) (2003) 635–644.
[17] A. Mehrabian, Pleasure-arousal-dominance: A general framework for describing
and measuring individual differences in temperament, Curr. Psychol. 14 (1996)
261–292.
[18] V. Tran, Positive affect negative affect scale (PANAS), in: Encyclopedia of
Behavioral Medicine, Springer, 2020, pp. 1708–1709.
[19] M.M. Bradley, P.J. Lang, Measuring emotion: The self-assessment manikin and the semantic differential, J. Behav. Ther. Exp. Psychiatry 25 (1) (1994) 49–59, http://dx.doi.org/10.1016/0005-7916(94)90063-9, URL https://www. sciencedirect.com/science/article/pii/0005791694900639.
[20] J.P. Pollak, P. Adams, G. Gay, PAM: A photographic affect meter for frequent, in situ measurement of affect, CHI ’11, Association for Computing Machinery, New York, NY, USA, 2011, pp. 725–734, http://dx.doi.org/10.1145/1978942. 1979047.
[21] S. Kang, C.Y. Park, A. Kim, N. Cha, U. Lee, Understanding emotion changes in mobile experience sampling, CHI ’22, Association for Computing Machinery, New York, NY, USA, 2022, http://dx.doi.org/10.1145/3491102.3501944.
[22] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, X. Yang, A review of emotion recognition using physiological signals, Sensors 18 (7) (2018) 2074.
[23] H. Perry Fordson, X. Xing, K. Guo, X. Xu, Emotion recognition with knowledge graph based on electrodermal activity, Front. Neurosci. 16 (2022) 911767.
[24] F. Larradet, R. Niewiadomski, G. Barresi, D.G. Caldwell, L.S. Mattos, Toward emotion recognition from physiological signals in the wild: approaching the methodological issues in real-life data collection, Front. Psychol. 11 (2020) 1111.
[25] D. Grühn, N. Sharifian, 7 - Lists of emotional stimuli, in: H.L. Meisel- man (Ed.), Emotion Measurement, Woodhead Publishing, 2016, pp. 145–164, http://dx.doi.org/10.1016/B978- 0- 08- 100508- 8.00007- 2, URL https://www. sciencedirect.com/science/article/pii/B9780081005088000072.
[26] G.N. Yannakakis, A. Paiva, Emotion in games, in: Handbook on Affective Computing, Vol. 2014, Oxford University Press, 2014, pp. 459–471.
[27] R. Somarathna, T. Bednarz, G. Mohammadi, Virtual reality for emotion elici- tation – a review, IEEE Trans. Affect. Comput. (2022) 1–21, http://dx.doi.org/ 10.1109/taffc.2022.3181053.
[28] M.A. Hasnul, N.A.A. Aziz, S. Alelyani, M. Mohana, A.A. Aziz, Electrocardiogram- based emotion recognition systems and their applications in healthcare—A review, Sensors 21 (15) (2021) http://dx.doi.org/10.3390/s21155015, URL https://www.mdpi.com/1424- 8220/21/15/5015.
[29] P.J. Bota, C. Wang, A.L.N. Fred, H. Plácido Da Silva, A review, current challenges, and future possibilities on emotion recognition using machine learning and physiological signals, IEEE Access 7 (2019) 140990–141020, http://dx.doi.org/10.1109/ACCESS.2019.2944001.
[30] Y.B. Singh, S. Goel, A systematic literature review of speech emotion recognition approaches, Neurocomputing 492 (2022) 245–263, http://dx.doi.org/10.1016/ j.neucom.2022.04.028, URL https://www.sciencedirect.com/science/article/pii/ S0925231222003964.
[31] K. Kamble, J. Sengupta, A comprehensive survey on emotion recognition based on electroencephalograph (EEG) signals, Multimedia Tools Appl. (2023) 1–36.
[32] J. Zhang, Z. Yin, P. Chen, S. Nichele, Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review, Inf. Fusion 59 (2020) 103–126, http://dx.doi.org/10.1016/j.inffus.2020.01.011, URL https:
//www.sciencedirect.com/science/article/pii/S1566253519302532.
[33] R.R. Adyapady, B. Annappa, A comprehensive review of facial expression
recognition techniques, Multimedia Syst. 29 (1) (2023) 73–103.
[34] S. Ba, X. Hu, Measuring emotions in education using wearable devices: A systematic review, Comput. Educ. 200 (2023) 104797, http://dx.doi.org/10. 1016/j.compedu.2023.104797.
[35] D. Moher, A. Liberati, J. Tetzlaff, D.G. Altman, P. Group*, Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement, Ann. Intern. Med. 151 (4) (2009) 264–269.
[36] S.K. Khare, V. Bajaj, G.R. Sinha, Automatic drowsiness detection based on variational non-linear chirp mode decomposition using electroencephalogram signals, in: Modelling and Analysis of Active Biopotential Signals in Healthcare, Volume 1, in: 2053-2563, IOP Publishing, 2020, http://dx.doi.org/10.1088/ 978-0-7503-3279-8ch5, 5–1 to 5–25.
[37] S.K. Khare, V. Bajaj, A self-learned decomposition and classification model for schizophrenia diagnosis, Comput. Methods Programs Biomed. 211 (2021) 106450, http://dx.doi.org/10.1016/j.cmpb.2021.106450, URL https://www. sciencedirect.com/science/article/pii/S0169260721005241.
[38] S.K. Khare, N.B. Gaikwad, V. Bajaj, VHERS: A novel variational mode decompo- sition and Hilbert transform-based EEG rhythm separation for automatic ADHD detection, IEEE Trans. Instrum. Meas. 71 (2022) 1–10, http://dx.doi.org/10. 1109/TIM.2022.3204076.
[39] S.K. Khare, S. March, P.D. Barua, V.M. Gadre, U.R. Acharya, Application of data fusion for automated detection of children with developmental and mental disorders: A systematic review of the last decade, Inf. Fusion 99 (2023) 101898, http://dx.doi.org/10.1016/j.inffus.2023.101898, URL https:// www.sciencedirect.com/science/article/pii/S1566253523002142.
[40] A.H. Krishna, A.B. Sri, K.Y.V.S. Priyanka, S. Taran, V. Bajaj, Emotion clas- sification using EEG signals based on tunable-q wavelet transform, IET Sci. Meas. Technol. 13 (3) (2019) 375–380, http://dx.doi.org/10.1049/iet-smt. 2018.5237, arXiv:https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/ iet- smt.2018.5237. URL https://ietresearch.onlinelibrary.wiley.com/doi/abs/ 10.1049/iet- smt.2018.5237.
[41] K.S. Kamble, J. Sengupta, Ensemble machine learning-based affective computing for emotion recognition using dual-decomposed EEG signals, IEEE Sens. J. 22 (3) (2022) 2496–2507, http://dx.doi.org/10.1109/JSEN.2021.3135953.
[42] P. Pandey, K. Seeja, Subject independent emotion recognition from EEG using VMD and deep learning, J. King Saud Univ. - Comput. Inf. Sci. 34 (5) (2022) 1730–1738, http://dx.doi.org/10.1016/j.jksuci.2019.11.003, URL https://www. sciencedirect.com/science/article/pii/S1319157819309991.
[43] Z. Mohammadi, J. Frounchi, M. Amiri, Wavelet-based emotion recognition system using EEG signal, Neural Comput. Appl. 28 (2017) 1985–1990.
[44] T. Chen, S. Ju, F. Ren, M. Fan, Y. Gu, EEG emotion recognition model based on the LIBSVM classifier, Measurement 164 (2020) 108047, http://dx. doi.org/10.1016/j.measurement.2020.108047, URL https://www.sciencedirect. com/science/article/pii/S0263224120305856.
[45] Y. Zhang, X. Ji, S. Zhang, An approach to EEG-based emotion recogni- tion using combined feature extraction method, Neurosci. Lett. 633 (2016) 152–157, http://dx.doi.org/10.1016/j.neulet.2016.09.037, URL https://www. sciencedirect.com/science/article/pii/S0304394016307200.
[46] S.K. Khare, V. Bajaj, An evolutionary optimized variational mode decomposition for emotion recognition, IEEE Sens. J. 21 (2) (2021) 2035–2042, http://dx.doi. org/10.1109/JSEN.2020.3020915.
[47] S.K. Khare, V. Bajaj, G.R. Sinha, Adaptive tunable Q wavelet transform-based emotion identification, IEEE Trans. Instrum. Meas. 69 (12) (2020) 9609–9617, http://dx.doi.org/10.1109/TIM.2020.3006611.
[48] V. Gupta, M.D. Chopda, R.B. Pachori, Cross-subject emotion recognition using flexible analytic wavelet transform from EEG signals, IEEE Sens. J. 19 (6) (2019) 2266–2274, http://dx.doi.org/10.1109/JSEN.2018.2883497.
[49] C. Wei, L. lan Chen, Z. zhen Song, X. guang Lou, D. dong Li, EEG-based emotion recognition using simple recurrent units network and ensemble learn- ing, Biomed. Signal Process. Control 58 (2020) 101756, http://dx.doi.org/10. 1016/j.bspc.2019.101756, URL https://www.sciencedirect.com/science/article/ pii/S1746809419303374.
[50] T. Tuncer, S. Dogan, A. Subasi, A new fractal pattern feature generation function based emotion recognition method using EEG, Chaos Solitons Fractals 144 (2021) 110671, http://dx.doi.org/10.1016/j.chaos.2021.110671, URL https:// www.sciencedirect.com/science/article/pii/S0960077921000242.
[51] P. V., A. Bhattacharyya, Human emotion recognition based on time– frequency analysis of multivariate EEG signal, Knowl.-Based Syst. 238 (2022) 107867, http://dx.doi.org/10.1016/j.knosys.2021.107867, URL https://www. sciencedirect.com/science/article/pii/S0950705121010455.
[52] N. Zhuang, Y. Zeng, L. Tong, C. Zhang, H. Zhang, B. Yan, Emotion recognition from EEG signals using multidimensional information in EMD domain, BioMed Res. Int. 2017 (2017).
[53] T. Song, W. Zheng, C. Lu, Y. Zong, X. Zhang, Z. Cui, MPED: A multi-modal physiological emotion database for discrete emotion recognition, IEEE Access 7 (2019) 12177–12191, http://dx.doi.org/10.1109/ACCESS.2019.2891579.
[54] A. Dogan, M. Akay, P.D. Barua, M. Baygin, S. Dogan, T. Tuncer, A.H. Dogru, U.R. Acharya, PrimePatNet87: Prime pattern and tunable q-factor wavelet transform techniques for automated accurate EEG emo- tion recognition, Comput. Biol. Med. 138 (2021) 104867, http://dx.doi. org/10.1016/j.compbiomed.2021.104867, URL https://www.sciencedirect.com/ science/article/pii/S0010482521006612.
[55] E. Deniz, N. Sobahi, N. Omar, A. Sengur, U.R. Acharya, Automated robust human emotion classification system using hybrid EEG features with ICBrainDB dataset, Health Inf. Sci. Syst. 10 (1) (2022) 31.
[56] M.R. Islam, M.M. Islam, M.M. Rahman, C. Mondal, S.K. Singha, M. Ahmad, A. Awal, M.S. Islam, M.A. Moni, EEG channel correlation based model for emotion recognition, Comput. Biol. Med. 136 (2021) 104757, http://dx.doi. org/10.1016/j.compbiomed.2021.104757, URL https://www.sciencedirect.com/ science/article/pii/S0010482521005515.
[57] F. Wang, S. Wu, W. Zhang, Z. Xu, Y. Zhang, C. Wu, S. Coleman, Emotion recognition with convolutional neural network and EEG-based EFDMs, Neuropsychologia 146 (2020) 107506, http://dx.doi.org/10.1016/j. neuropsychologia.2020.107506, URL https://www.sciencedirect.com/science/ article/pii/S0028393220301780.
[58] S.K. Khare, V. Bajaj, Time–frequency representation and convolutional neural network-based emotion recognition, IEEE Trans. Neural Netw. Learn. Syst. 32 (7) (2021) 2901–2909, http://dx.doi.org/10.1109/TNNLS.2020.3008938.
[59] P. Li, H. Liu, Y. Si, C. Li, F. Li, X. Zhu, X. Huang, Y. Zeng, D. Yao, Y. Zhang, P. Xu, EEG based emotion recognition by combining functional connectivity network and local activations, IEEE Trans. Biomed. Eng. 66 (10) (2019) 2869–2881, http://dx.doi.org/10.1109/TBME.2019.2897651.
[60] X. Du, C. Ma, G. Zhang, J. Li, Y.-K. Lai, G. Zhao, X. Deng, Y.-J. Liu, H. Wang, An efficient LSTM network for emotion recognition from multichannel EEG signals, IEEE Trans. Affect. Comput. 13 (3) (2022) 1528–1540, http: //dx.doi.org/10.1109/TAFFC.2020.3013711.
[61] S. Khare, A. Nishad, A. Upadhyay, V. Bajaj, Classification of emo- tions from EEG signals using time-order representation based on the S-transform and convolutional neural network, Electron. Lett. 56 (25) (2020) 1359–1361, http://dx.doi.org/10.1049/el.2020.2380, arXiv:https:// ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/el.2020.2380. URL https: //ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/el.2020.2380.
[62] R. Alazrai, R. Homoud, H. Alwanni, M.I. Daoud, EEG-based emotion recog- nition using quadratic time-frequency distribution, Sensors 18 (8) (2018) http: //dx.doi.org/10.3390/s18082739, URL https://www.mdpi.com/1424-8220/18/ 8/2739.
[63] A. Topic, M. Russo, Emotion recognition based on EEG feature maps through deep learning network, Eng. Sci. Technol. Int. J. 24 (6) (2021) 1442–1454, http://dx.doi.org/10.1016/j.jestch.2021.03.012, URL https://www. sciencedirect.com/science/article/pii/S2215098621000768.
[64] S. Hwang, K. Hong, G. Son, H. Byun, Learning CNN features from DE features for EEG-based emotion recognition, Pattern Anal. Appl. 23 (2020) 1323–1335.
[65] A. Sepúlveda, F. Castillo, C. Palma, M. Rodriguez-Fernandez, Emotion recogni- tion from ECG signals using wavelet scattering and machine learning, Appl. Sci. 11 (11) (2021) http://dx.doi.org/10.3390/app11114945, URL https://www.
mdpi.com/2076- 3417/11/11/4945.
[66] K.N. Minhad, S.H.M. Ali, M.B.I. Reaz, Happy-anger emotions classifications
from electrocardiogram signal for automobile driving safety and awareness, J. Transp. Health 7 (2017) 75–89, http://dx.doi.org/10.1016/j.jth.2017.11.001, Road Danger Reduction. URL https://www.sciencedirect.com/science/article/ pii/S2214140516303693.
[67] R. Subramanian, J. Wache, M.K. Abadi, R.L. Vieriu, S. Winkler, N. Sebe, ASCERTAIN: Emotion and personality recognition using commercial sensors, IEEE Trans. Affect. Comput. 9 (2) (2018) 147–160, http://dx.doi.org/10.1109/ TAFFC.2016.2625250.
[68] K. NISA’MINHAD, S.H.M. Ali, M.B.I. Reaz, A design framework for human emotion recognition using electrocardiogram and skin conductance response signals, J. Eng. Sci. Technol. 12 (11) (2017) 3102–3119.
[69] J. Selvaraj, M. Murugappan, K. Wan, S. Yaacob, Classification of emotional states from electrocardiogram signals: a non-linear approach based on hurst, Biomed. Eng. Online 12 (1) (2013) 1–18.
[70] S.-T. Pan, W.-C. Li, Fuzzy-HMM modeling for emotion detection us- ing electrocardiogram signals, Asian J. Control 22 (6) (2020) 2206– 2216, http://dx.doi.org/10.1002/asjc.2375, arXiv:https://onlinelibrary.wiley. com/doi/pdf/10.1002/asjc.2375. URL https://onlinelibrary.wiley.com/doi/abs/ 10.1002/asjc.2375.
[71] T. Chen, H. Yin, X. Yuan, Y. Gu, F. Ren, X. Sun, Emotion recognition based on fusion of long short-term memory networks and SVMs, Digit. Signal Process. 117 (2021) 103153, http://dx.doi.org/10.1016/j.dsp.2021.103153, URL https: //www.sciencedirect.com/science/article/pii/S1051200421001925.
[72] M. Nardelli, G. Valenza, A. Greco, A. Lanata, E.P. Scilingo, Recognizing emotions induced by affective sounds through heart rate variability, IEEE Trans. Affect. Comput. 6 (4) (2015) 385–394, http://dx.doi.org/10.1109/TAFFC.2015. 2432810.
[73] S. Nita, S. Bitam, M. Heidet, A. Mellouk, A new data augmentation con- volutional neural network for human emotion recognition based on ECG signals, Biomed. Signal Process. Control 75 (2022) 103580, http://dx.doi. org/10.1016/j.bspc.2022.103580, URL https://www.sciencedirect.com/science/ article/pii/S1746809422001021.
[74] F.E. Oğuz, A. Alkan, T. Schöler, Emotion detection from ECG signals with different learning algorithms and automated feature engineering, Signal Image Video Process. (2023) 1–9.
[75] Y.-L. Hsu, J.-S. Wang, W.-C. Chiang, C.-H. Hung, Automatic ECG-based emotion recognition in music listening, IEEE Trans. Affect. Comput. 11 (1) (2020) 85–99, http://dx.doi.org/10.1109/TAFFC.2017.2781732.
[76] J. S, M. Murugappan, K. Wan, S. Yaacob, Electrocardiogram-based emotion recognition system using empirical mode decomposition and discrete Fourier transform, Expert Syst. 31 (2) (2014) 110–120, http://dx.doi.org/10.1111/exsy. 12014.
[77] An accurate emotion recognition system using ECG and GSR signals and matching pursuit method.
[78] T. Dissanayake, Y. Rajapaksha, R. Ragel, I. Nawinne, An ensemble learning approach for electrocardiogram sensor based human emotion recognition, Sensors 19 (20) (2019) http://dx.doi.org/10.3390/s19204495, URL https:// www.mdpi.com/1424- 8220/19/20/4495.
[79] D.S. Hammad, H. Monkaresi, ECG-based emotion detection via parallel- extraction of temporal and spatial features using convolutional neural network, Trait. Signal 39 (1) (2022).
[80] T. Fan, S. Qiu, Z. Wang, H. Zhao, J. Jiang, Y. Wang, J. Xu, T. Sun, N. Jiang, A new deep convolutional neural network incorporating attentional mechanisms for ECG emotion recognition, Comput. Biol. Med. 159 (2023) 106938, http://dx.doi.org/10.1016/j.compbiomed.2023.106938, URL https:// www.sciencedirect.com/science/article/pii/S0010482523004031.
[81] A.N. Khan, A.A. Ihalage, Y. Ma, B. Liu, Y. Liu, Y. Hao, Deep learning framework for subject-independent emotion detection using wireless signals, PLOS ONE 16 (2) (2021) 1–16, http://dx.doi.org/10.1371/journal.pone.0242946.
[82] M.N. Dar, M.U. Akram, S.G. Khawaja, A.N. Pujari, CNN and LSTM-based emotion charting using physiological signals, Sensors 20 (16) (2020) http: //dx.doi.org/10.3390/s20164551, URL https://www.mdpi.com/1424-8220/20/ 16/4551.
[83] P. Sarkar, A. Etemad, Self-supervised ECG representation learning for emotion recognition, IEEE Trans. Affect. Comput. 13 (3) (2022) 1541–1554, http://dx. doi.org/10.1109/TAFFC.2020.3014842.
[84] A. Raheel, M. Majid, M. Alnowami, S.M. Anwar, Physiological sensors based emotion recognition while experiencing tactile enhanced multimedia, Sensors 20 (14) (2020) http://dx.doi.org/10.3390/s20144037, URL https://www.mdpi. com/1424- 8220/20/14/4037.
[85] D. Ayata, Y. Yaslan, M. Kamasak, Emotion recognition via galvanic skin response: Comparison of machine learning algorithms and feature extraction methods, Istanb. Univ. - J. Electr. Electron. Eng. 17 (2017) ISSN: 1303–0914.
[86] Application of fractional Fourier transform in feature extraction from ELECTROCARDIOGRAM and GALVANIC SKIN RESPONSE for emotion recognition.
[87] A. Goshvarpour, A. Goshvarpour, The potential of photoplethysmogram and galvanic skin response in emotion recognition using nonlinear features, Phys. Eng. Sci. Med. 43 (1) (2020) 119–134.
[88] C. Li, C. Xu, Z. Feng, Analysis of physiological for emotion recognition with the IRS model, Neurocomputing 178 (2016) 103–111, http://dx.doi.org/10. 1016/j.neucom.2015.07.112, Smart Computing for Large Scale Visual Data Sensing and Processing. URL https://www.sciencedirect.com/science/article/ pii/S0925231215016045.
[89] A Shrewd Artificial Neural Network-Based Hybrid Model for Pervasive Stress Detection of Students Using Galvanic Skin Response and Electrocardiogram Signals.
[90] A. Goshvarpour, A. Abbasi, A. Goshvarpour, S. Daneshvar, Discrimination between different emotional states based on the chaotic behavior of galvanic skin responses, Signal Image Video Process. 11 (2017) 1347–1355.
[91] J. Domínguez-Jiménez, K. Campo-Landines, J. Martínez-Santos, E. Delahoz, S. Contreras-Ortiz, A machine learning model for emotion recognition from physiological signals, Biomed. Signal Process. Control 55 (2020) 101646, http://dx.doi.org/10.1016/j.bspc.2019.101646, URL https://www.sciencedirect. com/science/article/pii/S1746809419302277.
[92] Fusion framework for emotional electrocardiogram and galvanic skin response recognition: Applying wavelet transform.
[93] A. Goshvarpour, A. Abbasi, A. Goshvarpour, S. Daneshvar, A novel signal- based fusion approach for accurate music emotion recognition, Biomed. Eng.: Appl. Basis Commun. 28 (06) (2016) 1650040, http://dx.doi.org/10.4015/ S101623721650040X, arXiv:https://doi.org/10.4015/S101623721650040X.
[94] X. Sun, T. Hong, C. Li, F. Ren, Hybrid spatiotemporal models for senti- ment classification via galvanic skin response, Neurocomputing 358 (2019) 385–400, http://dx.doi.org/10.1016/j.neucom.2019.05.061, URL https://www. sciencedirect.com/science/article/pii/S0925231219307672.
[95] D.-H. Kang, D.-H. Kim, 1D convolutional autoencoder-based PPG and GSR signals for real-time emotion classification, IEEE Access.
[96] Y. Li, J. Deng, Q. Wu, Y. Wang, Eye-tracking signals based affective classification employing deep gradient convolutional neural networks, 2021.
[97] V. Skaramagkas, E. Ktistakis, D. Manousos, E. Kazantzaki, N.S. Tachos, E. Tripoliti, D.I. Fotiadis, M. Tsiknakis, eSEE-d: Emotional state estimation based on eye-tracking dataset, Brain Sci. 13 (4) (2023) 589.
[98] N. Baharom, N. Jayabalan, M. Amin, S. Wibirama, Positive emotion recognition through eye tracking technology, J. Adv. Manuf. Technol. (JAMT) 13 (2(1)) (1 1). URL https://jamt.utem.edu.my/jamt/article/view/5683.
[99] D. Bethge, L. Chuang, T. Grosse-Puppendahl, Analyzing transferability of happiness detection via gaze tracking in multimedia applications, in: ACM Symposium on Eye Tracking Research and Applications, in: ETRA ’20 Adjunct, Association for Computing Machinery, New York, NY, USA, 2020, http://dx. doi.org/10.1145/3379157.3391655.
[100] Y. Stylianou, Voice transformation: A survey, in: 2009 IEEE International Conference on Acoustics, Speech and Signal Processing, 2009, pp. 3585–3588, http://dx.doi.org/10.1109/ICASSP.2009.4960401.
[101] A. Christy, S. Vaithyasubramanian, J. A., M. Praveena, Multimodal speech emotion recognition and classification using convolutional neural network techniques, Int. J. Speech Technol. 23 (2020) 381–388, http://dx.doi.org/10. 1007/s10772- 020- 09713- y.
[102] M. Jain, S. Narayan, P. Balaji, B.K. P, A. Bhowmick, K. R, R.K. Muthu, Speech emotion recognition using support vector machine, 2020, arXiv:2002.07590.
[103] A. Koduru, H. Valiveti, A. Budati, Feature extraction algorithms to improve the speech emotion recognition rate, Int. J. Speech Technol. 23 (2020) 45–55, http://dx.doi.org/10.1007/s10772- 020- 09672- 4.
[104] M. Ren, W. Nie, A. Liu, Y. Su, Multi-modal Correlated Network for emotion recognition in speech, Vis. Inform. 3 (3) (2019) 150–155, http://dx.doi.org/10. 1016/j.visinf.2019.10.003, URL https://www.sciencedirect.com/science/article/ pii/S2468502X19300488.
[105] Z. Yang, Y. Huang, Algorithm for speech emotion recognition classification based on Mel-frequency Cepstral coefficients and broad learning system, Evol. Intell. 15 (2021) 2485–2494, http://dx.doi.org/10.1007/s12065-020-00532-3.
[106] K. Wang, N. An, B.N. Li, Y. Zhang, L. Li, Speech emotion recognition using Fourier parameters, IEEE Trans. Affect. Comput. 6 (1) (2015) 69–75, http: //dx.doi.org/10.1109/TAFFC.2015.2392101.
[107] S. Tripathi, A. Kumar, A. Ramesh, C. Singh, P. Yenigalla, Deep learning based emotion recognition system using speech features and transcriptions, 2019, arXiv:1906.05681.
[108] A. Bhavan, P. Chauhan, Hitkul, R.R. Shah, Bagged support vector ma- chines for emotion recognition from speech, Knowl.-Based Syst. 184 (2019) 104886, http://dx.doi.org/10.1016/j.knosys.2019.104886, URL https://www. sciencedirect.com/science/article/pii/S0950705119303533.
[109] Z.-T. Liu, Q. Xie, M. Wu, W.-H. Cao, Y. Mei, J.-W. Mao, Speech emotion recognition based on an improved brain emotion learning model, Neurocom- puting 309 (2018) 145–156, http://dx.doi.org/10.1016/j.neucom.2018.05.005, URL https://www.sciencedirect.com/science/article/pii/S0925231218305344.
[110] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, X. Li, Learning alignment for multimodal emotion recognition from speech, 2020, arXiv:1909.05645.
[111] M. Farooq, F. Hussain, N.K. Baloch, F.R. Raja, H. Yu, Y.B. Zikria, Impact
of feature selection algorithm on speech emotion recognition using deep convolutional neural network, Sensors 20 (21) (2020) http://dx.doi.org/10. 3390/s20216008, URL https://www.mdpi.com/1424-8220/20/21/6008.
[112] Mustaqeem, S. Kwon, Optimal feature selection based speech emotion recogni- tion using two-stream deep convolutional neural network, Int. J. Intell. Syst. 36 (9) (2021) 5116–5135, http://dx.doi.org/10.1002/int.22505, arXiv:https:// onlinelibrary.wiley.com/doi/pdf/10.1002/int.22505. URL https://onlinelibrary. wiley.com/doi/abs/10.1002/int.22505.
[113] D. Issa, M. Fatih Demirci, A. Yazici, Speech emotion recognition with deep con- volutional neural networks, Biomed. Signal Process. Control 59 (2020) 101894, http://dx.doi.org/10.1016/j.bspc.2020.101894, URL https://www.sciencedirect. com/science/article/pii/S1746809420300501.
[114] Mustaqeem, S. Kwon, A CNN-assisted enhanced audio signal processing for speech emotion recognition, Sensors 20 (1) (2020) http://dx.doi.org/10.3390/ s20010183, URL https://www.mdpi.com/1424- 8220/20/1/183.
[115] A. Bakhshi, A. Harimi, S. Chalup, CyTex: Transforming speech to textured im- ages for speech emotion recognition, Speech Commun. 139 (2022) 62–75, http: //dx.doi.org/10.1016/j.specom.2022.02.007, URL https://www.sciencedirect. com/science/article/pii/S0167639322000310.
[116] A. Badshah, N. Rahim, N. Ullah, J. Ahmad, K. Muhammad, M. Lee, S. Kwon, S. Baik, Deep features-based speech emotion recognition for smart affective services, Multimedia Tools Appl. 78 (2019) 5571–5589, http://dx.doi.org/10.1007/s11042-017-5292-7.
[117] S. Zhang, S. Zhang, T. Huang, W. Gao, Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching, IEEE Trans. Multimed. 20 (6) (2018) 1576–1590, http://dx.doi.org/10.1109/TMM.2017.2766843.
[118] Y. Xie, R. Liang, Z. Liang, C. Huang, C. Zou, B. Schuller, Speech emotion classification using attention-based LSTM, IEEE/ACM Trans. Audio Speech Lang. Process. 27 (11) (2019) 1675–1685, http://dx.doi.org/10.1109/TASLP.2019.2925934.
[119] Mustaqeem, M. Sajjad, S. Kwon, Clustering-based speech emotion recognition by incorporating learned features and deep BiLSTM, IEEE Access 8 (2020) 79861–79875, http://dx.doi.org/10.1109/ACCESS.2020.2990405.
[120] S. Kanwal, S. Asghar, Speech emotion recognition using clustering based GA- optimized feature set, IEEE Access 9 (2021) 125830–125842, http://dx.doi.org/10.1109/ACCESS.2021.3111659.
[121] Z. Xiao, E. Dellandréa, W. Dou, L. Chen, Multi-stage classification of emotional speech motivated by a dimensional emotion model, Multimedia Tools Appl. 46 (2010) 119–145, http://dx.doi.org/10.1007/s11042-009-0319-3.
[122] H.A. Shehu, W.N. Browne, H. Eisenbarth, An anti-attack method for emotion categorization from images, Appl. Soft Comput. 128 (2022) 109456, http://dx.doi.org/10.1016/j.asoc.2022.109456, URL [...].
[123] S. Kuruvayil, S. Palaniswamy, Emotion recognition from facial images with simultaneous occlusion, pose and illumination variations using meta-learning, J. King Saud Univ. - Comput. Inf. Sci. 34 (9) (2022) 7271–7282, http://dx.doi.org/10.1016/j.jksuci.2021.06.012, URL [...].
[124] I. Haider, H.-J. Yang, G.-S. Lee, S.-H. Kim, Robust human face emotion classification using triplet-loss-based deep CNN features and SVM, Sensors 23 (10) (2023) http://dx.doi.org/10.3390/s23104770, URL [...].
[125] D. Sen, S. Datta, R. Balasubramanian, Facial emotion classification using concatenated geometric and textural features, Multimedia Tools Appl. 78 (2019) 10287–10323, http://dx.doi.org/10.1007/s11042-018-6537-9.
[126] J. Deng, G. Pang, Z. Zhang, Z. Pang, H. Yang, G. Yang, cGAN based facial expression recognition for human-robot interaction, IEEE Access 7 (2019) 9848–9859, http://dx.doi.org/10.1109/ACCESS.2019.2891668.
[127] A.K. Hassan, S.N. Mohammed, A novel facial emotion recognition scheme based on graph mining, Def. Technol. 16 (5) (2020) 1062–1072, http://dx.doi.org/10.1016/j.dt.2019.12.006, URL [...].
[128] J.-H. Kim, B.-G. Kim, P.P. Roy, D.-M. Jeong, Efficient facial expression recogni- tion algorithm based on hierarchical deep neural network structure, IEEE Access 7 (2019) 41273–41285, http://dx.doi.org/10.1109/ACCESS.2019.2907327.
[129] J. Li, K. Jin, D. Zhou, N. Kubota, Z. Ju, Attention mechanism-based CNN for facial expression recognition, Neurocomputing 411 (2020) 340–350, http://dx.doi.org/10.1016/j.neucom.2020.06.014, URL [...].
[130] G. Tonguç, B. Ozaydın Ozkara, Automatic recognition of student emotions from facial expressions during a lecture, Comput. Educ. 148 (2020) 103797, http://dx.doi.org/10.1016/j.compedu.2019.103797, URL [...].
[131] S.L. Happy, A. Routray, Automatic facial expression recognition using features of salient facial patches, IEEE Trans. Affect. Comput. 6 (1) (2015) 1–12, http://dx.doi.org/10.1109/TAFFC.2014.2386334.
[132] P. Rodriguez, G. Cucurull, J. Gonzàlez, J.M. Gonfaus, K. Nasrollahi, T.B. Moes- lund, F.X. Roca, Deep pain: Exploiting long short-term memory networks for facial expression classification, IEEE Trans. Cybern. 52 (5) (2022) 3314–3324, http://dx.doi.org/10.1109/TCYB.2017.2662199.
[133] S. Minaee, M. Minaei, A. Abdolrashidi, Deep-emotion: Facial expression recog- nition using attentional convolutional network, Sensors 21 (9) (2021) http: //dx.doi.org/10.3390/s21093046, URL [...].
[134] W. Xiaohua, P. Muzi, P. Lijuan, H. Min, J. Chunhua, R. Fuji, Two-level attention with two-stage multi-task learning for facial emotion recognition, J. Vis. Commun. Image Represent. 62 (2019) 217–225, http://dx.doi.org/10. 1016/j.jvcir.2019.05.009, URL [...].
[135] T. Rao, M. Xu, D. Xu, Learning multi-level deep representations for image emotion classification, Neural Process. Lett. 51 (2016) 2043–2061.
[136] N. Sun, Q. Li, R. Huan, J. Liu, G. Han, Deep spatial-temporal feature fusion for facial expression recognition in static images, Pattern Recognit. Lett. 119 (2019) 49–61, http://dx.doi.org/10.1016/j.patrec.2017.10.022, Deep Learning for Pattern Recognition. URL [...].
[137] M.A.H. Akhand, S. Roy, N. Siddique, M.A.S. Kamal, T. Shimamura, Facial emotion recognition using transfer learning in the deep CNN, Electronics 10 (9) (2021) http://dx.doi.org/10.3390/electronics10091036, URL [...].
[138] A. Khattak, M.Z. Asghar, M. Ali, U. Batool, An efficient deep learning tech- nique for facial emotion recognition, Multimedia Tools Appl. 81 (2) (2022) 1649–1683, http://dx.doi.org/10.1007/s11042-021-11298-w.
[139] M. Maithri, U. Raghavendra, A. Gudigar, J. Samanth, P.D. Barua, M. Muru- gappan, Y. Chakole, U.R. Acharya, Automated emotion recognition: Current trends and future perspectives, Comput. Methods Programs Biomed. 215 (2022) 106646, http://dx.doi.org/10.1016/j.cmpb.2022.106646, URL [...].
[140] U. Raghavendra, A. Gudigar, Y. Chakole, P. Kasula, D.P. Subha, N.A. Kadri, E.J. Ciaccio, U.R. Acharya, Automated detection and screening of depres- sion using continuous wavelet transform with electroencephalogram signals, Expert Syst. 40 (4) (2023) e12803, http://dx.doi.org/10.1111/exsy.12803, arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/exsy.12803. URL https: //onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12803.
[141] M.N. Dar, M.U. Akram, R. Yuvaraj, S. Gul Khawaja, M. Murugappan, EEG-based emotion charting for Parkinson’s disease patients using Convolutional Recurrent Neural Networks and cross dataset learning, Comput. Biol. Med. 144 (2022) 105327, http://dx.doi.org/10.1016/j.compbiomed.2022.105327, URL [...].
[142] M. Murugappan, W. Alshuaib, A.K. Bourisly, S.K. Khare, S. Sruthi, V. Bajaj, Tunable Q wavelet transform based emotion classification in Parkinson’s disease using Electroencephalography, PLOS ONE 15 (11) (2020) 1–17, http://dx.doi. org/10.1371/journal.pone.0242014.
[143] S. Righi, G. Gronchi, S. Ramat, G. Gavazzi, F. Cecchi, M.P. Viggiano, Automatic and controlled attentional orienting toward emotional faces in patients with Parkinson’s disease, Cogn. Affect. Behav. Neurosci. 23 (2) (2023) 371–382.
[144] J. Skibińska, R. Burget, Parkinson’s disease detection based on changes of emotions during speech, in: 2020 12th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT), 2020, pp. 124–130, http://dx.doi.org/10.1109/ICUMT51630.2020.9222446.
[145] W.-L. Chu, M.-W. Huang, B.-L. Jian, K.-S. Cheng, Analysis of EEG entropy during visual evocation of emotion in schizophrenia, Ann. Gen. Psychiatry 16 (2017) 1–9.
[146] D.I. Leitman, P. Laukka, P.N. Juslin, E. Saccente, P. Butler, D.C. Javitt, Getting the cue: Sensory contributions to auditory emotion recog- nition impairments in schizophrenia, Schizophr. Bull. 36 (3) (2008) 545–556, http://dx.doi.org/10.1093/schbul/sbn115, arXiv:https://academic. oup.com/schizophreniabulletin/article-pdf/36/3/545/5311926/sbn115.pdf.
[147] M.K. Mandal, U. Habel, R.C. Gur, Facial expression-based indicators of schizophrenia: Evidence from recent research, Schizophr. Res. 252 (2023) 335–344, http://dx.doi.org/10.1016/j.schres.2023.01.016, URL [...].
[148] N. Liu, Z. Yuan, Y. Chen, C. Liu, L. Wang, Learning implicit sentiments in Alzheimer’s disease recognition with contextual attention features, Front. Aging Neurosci. 15 (2023) 1122799.
[149] W. Maturana, I. Lobo, J. Landeira-Fernandez, D.C. Mograbi, Nondeclarative associative learning in Alzheimer’s disease: An overview of eyeblink, fear, and other emotion-based conditioning, Physiol. Behav. 268 (2023) 114250, http: //dx.doi.org/10.1016/j.physbeh.2023.114250, URL [...].
[150] I. Ferrer-Cairols, L. Ferré-González, G. García-Lluch, C. Peña-Bautista, L. Álvarez-Sánchez, M. Baquero, C. Cháfer-Pericás, Emotion recognition and baseline cortisol levels relationship in early Alzheimer disease, Biol. Psychol. 177 (2023) 108511, http://dx.doi.org/10.1016/j.biopsycho.2023.108511, URL [...].
[151] M. Brandt, F. de Oliveira Silva, J.P.S. Neto, M.A.T. Baptista, T. Belfort, I.B. Lacerda, M.C.N. Dourado, Facial expression recognition of emotional situations in mild and moderate Alzheimer’s disease, J. Geriatr. Psychiatry Neurol. 0 (0) (0) 08919887231175432. PMID: 37160761. http://dx.doi.org/10.1177/ 08919887231175432.
[152] S. Gupta, A. Singh, J. Ranjan, Multimodal, multiview and multitasking de- pression detection framework endorsed with auxiliary sentiment polarity and emotion detection, Int. J. Syst. Assur. Eng. Manag. (2023) 1–16.
[153] M. Tadalagi, A.M. Joshi, AutoDep: automatic depression detection using facial expressions based on linear binary pattern descriptor, Med. Biol. Eng. Comput. 59 (6) (2021) 1339–1354.
[154] H. Chang, Y. Zong, W. Zheng, C. Tang, J. Zhu, X. Li, Depression assessment method: an EEG emotion recognition framework based on spatiotemporal neural network, Front. Psychiatry 12 (2022) 837149.
[155] Ü. Aydin, R. Cañigueral, C. Tye, G. McLoughlin, Face processing in young adults with autism and ADHD: An event related potentials study, Front. Psychiatry 14 (2023) 1080681.
[156] L. Sacco, L. Morellini, C. Cerami, The diagnosis and the therapy of social cognition deficits in adults affected by ADHD and MCI, Front. Neurol. 14 (2023) 1162510.
[157] E. McKay, K. Cornish, H. Kirk, Impairments in emotion recognition and positive emotion regulation predict social difficulties in adolescent with ADHD, Clin. Child Psychol. Psychiatry 28 (3) (2023) 895–908, DOI: 10.1177/13591045221141770, PMID: 36440882.
[158] S. Le Sourn-Bissaoui, M. Aguert, P. Girard, C. Chevreuil, V. Laval, Emotional speech comprehension in children and adolescents with autism spectrum disorders, J. Commun. Disord. 46 (4) (2013) 309–320, DOI: 10.1016/j.jcomdis.2013.03.002.
[159] R. Matin, D. Valles, A speech emotion recognition solution-based on support vector machine for children with autism spectrum disorder to help identify human emotions, in: 2020 Intermountain Engineering, Technology and Computing (IETC), 2020, pp. 1–6, [DOI: 10.1109/IETC47856.2020.9249147](http://dx.doi.org/10.1109/IETC47856.2020.9249147).
[160] M. Derbali, M. Jarrah, P. Randhawa, Autism spectrum disorder detection: Video games based facial expression diagnosis using deep learning, Int. J. Adv. Comput. Sci. Appl. 14 (1) (2023).
[161] N.F. Harun, N. Hamzah, N. Zaini, M.M. Sani, H. Norhazman, I.M. Yassin, EEG classification analysis for diagnosing autism spectrum disorder based on emotions, J. Telecommun. Electron. Comput. Eng. (JTEC) 10 (1–2) (2018) 87–93.
[162] S. Pick, J.D. Mellers, L.H. Goldstein, Explicit facial emotion processing in patients with dissociative seizures, Psychosom. Med. 78 (7) (2016) 874–885.
[163] J. Amlerova, A.E. Cavanna, O. Bradac, A. Javurkova, J. Raudenska, P. Marusic, Emotion recognition and social cognition in temporal lobe epilepsy and the effect of epilepsy surgery, Epilepsy Behav. 36 (2014) 86–89, [DOI: 10.1016/j.yebeh.2014.05.001](http://dx.doi.org/10.1016/j.yebeh.2014.05.001).
[164] L.W. Carawan, B.A. Nalavany, C. Jenkins, Emotional experience with dyslexia and self-esteem: the protective role of perceived family support in late adulthood, Aging Ment. Health 20 (3) (2016) 284–294, DOI: 10.1080/13607863.2015.1008984, PMID: 25660279.
[165] E. Anyanwu, A. Campbell, Childhood emotional experiences leading to biopsychosocially-induced dyslexia and low academic performance in adolescence, Int. J. Adolesc. Med. Health 13 (3) (2001) 191–204, DOI: 10.1515/IJAMH.2001.13.3.191.
[166] M. Doikou-Avlidou, The educational, social and emotional experiences of students with dyslexia: The perspective of postsecondary education students, Int. J. Spec. Educ. 30 (1) (2015) 132–145.
[167] P.M. Cole, J. Luby, M.W. Sullivan, Emotions and the development of childhood depression: Bridging the gap, Child Dev. Perspect. 2 (3) (2008) 141–148, DOI: 10.1111/j.1750-8606.2008.00056.x.
[168] S. Siener, K.A. Kerns, Emotion regulation and depressive symptoms in preadolescence, Child Psychiatry Hum. Dev. 43 (2012) 414–430.
[169] C. Suveg, J. Zeman, Emotion regulation in children with anxiety disorders, J. Clin. Child Adolesc. Psychol. 33 (4) (2004) 750–759, DOI: 10.1207/s15374424jccp3304_10, PMID: 15498742.
[170] D.K. Hannesdottir, T.H. Ollendick, The role of emotion regulation in the treatment of child anxiety disorders, Clin. Child Fam. Psychol. Rev. 10 (2007) 275–293.
[171] F.M. Talaat, Real-time facial emotion recognition system among children with autism based on deep learning and IoT, Neural Comput. Appl. 35 (17) (2023) 12717–12728.
[172] L. Berkovits, A. Eisenhower, J. Blacher, Emotion regulation in young children with autism spectrum disorders, J. Autism Dev. Disord. 47 (2017) 68–79.
[173] C. Ryan, C.N. Charragáin, Teaching emotion recognition skills to children with autism, J. Autism Dev. Disord. 40 (12) (2010) 1505–1511.
[174] V. Blanes-Vidal, J. Bælum, E.S. Nadimi, P. Løfstrøm, L.P. Christensen, Chronic exposure to odorous chemicals in residential areas and effects on human psychosocial health: Dose–response relationships, Sci. Total Environ. 490 (2014) 545–554, [DOI: 10.1016/j.scitotenv.2014.05.041](http://dx.doi.org/10.1016/j.scitotenv.2014.05.041).
[175] M.L. Cantuaria, J. Brandt, V. Blanes-Vidal, Exposure to multiple environmental stressors, emotional and physical well-being, and self-rated health: An analysis of relationships using latent variable structural equation modelling, Environ. Res. 227 (2023) 115770, [DOI: 10.1016/j.envres.2023.115770](http://dx.doi.org/10.1016/j.envres.2023.115770).
[176] P. Weichbroth, W. Sroka, A note on the affective computing systems and machines: A classification and appraisal, Procedia Comput. Sci. 207 (C) (2022) 3798–3807, [DOI: 10.1016/j.procs.2022.09.441](http://dx.doi.org/10.1016/j.procs.2022.09.441).
[177] D. Caruelle, P. Shams, A. Gustafsson, L. Lervik-Olsen, Affective computing in marketing: practical implications and research opportunities afforded by emotionally intelligent machines, Mark. Lett. 33 (1) (2022) 163–169.
[178] L. Cen, F. Wu, Z.L. Yu, F. Hu, Chapter 2 - A real-time speech emotion recognition system and its application in online learning, in: S.Y. Tettegah, M. Gartmeier (Eds.), Emotions, Technology, Design, and Learning, in: Emotions and Technology, Academic Press, San Diego, 2016, pp. 27–46, DOI: 10.1016/B978-0-12-801856-9.00002-5.
[179] M. Zembylas, M. Theodorou, A. Pavlakis, The role of emotions in the experience of online learning: Challenges and opportunities, Educ. Media Int. 45 (2) (2008) 107–117.
[180] O.S. Lih, V. Jahmunah, E.E. Palmer, P.D. Barua, S. Dogan, T. Tuncer, S. García, F. Molinari, U.R. Acharya, EpilepsyNet: Novel automated detection of epilepsy using transformer model with EEG signals from 121 patient population, Comput. Biol. Med. 164 (2023) 107312, DOI: 10.1016/j.compbiomed.2023.107312, URL.
[181] F. Panahi, S. Rashidi, A. Sheikhani, Application of fractional Fourier transform in feature extraction from ELECTROCARDIOGRAM and GALVANIC SKIN RESPONSE for emotion recognition, Biomed. Signal Process. Control 69 (2021) 102863, [DOI: 10.1016/j.bspc.2021.102863](http://dx.doi.org/10.1016/j.bspc.2021.102863), [URL](https://www.sciencedirect.com/science/article/pii/S1746809421004602).
[182] H.W. Loh, C.P. Ooi, S.L. Oh, P.D. Barua, Y.R. Tan, F. Molinari, S. March, U.R. Acharya, D.S.S. Fung, Deep neural network technique for automated detection of ADHD and CD using ECG signal, Comput. Methods Programs Biomed. 241 (2023) 107775, [DOI: 10.1016/j.cmpb.2023.107775](http://dx.doi.org/10.1016/j.cmpb.2023.107775), [URL](https://www.sciencedirect.com/science/article/pii/S0169260723004418).
[183] O. Faust, W. Hong, H.W. Loh, S. Xu, R.-S. Tan, S. Chakraborty, P.D. Barua, F. Molinari, U.R. Acharya, Heart rate variability for medical decision support systems: A review, Comput. Biol. Med. 145 (2022) 105407.
[184] H.W. Loh, S. Xu, O. Faust, C.P. Ooi, P.D. Barua, S. Chakraborty, R.-S. Tan, F. Molinari, U.R. Acharya, Application of photoplethysmography signals for healthcare systems: An in-depth review, Comput. Methods Programs Biomed. 216 (2022) 106677.
[185] J. Zhou, G. Fang, N. Wu, Survey on security and privacy-preserving in federated learning, J. Xihua Univ. (Nat. Sci. Ed.) 39 (4) (2020) 9–17.
[186] F. Liu, M. Li, X. Liu, T. Xue, J. Ren, C. Zhang, A review of federated meta-learning and its application in cyberspace security, Electronics 12 (15) (2023) 3295.
[187] A. Noore, R. Singh, M. Vasta, Fusion, sensor-level, in: S.Z. Li, A. Jain (Eds.), Encyclopedia of Biometrics, Springer US, Boston, MA, 2009, pp. 616–621, DOI: 10.1007/978-0-387-73003-5_156.
[188] A. Ross, Fusion, feature-level, in: S.Z. Li, A. Jain (Eds.), Encyclopedia of Biometrics, Springer US, Boston, MA, 2009, pp. 597–602, DOI: 10.1007/978-0-387-73003-5_157.
[189] L. Osadciw, K. Veeramachaneni, Fusion, decision-level, in: S.Z. Li, A. Jain (Eds.), Encyclopedia of Biometrics, Springer US, Boston, MA, 2009, pp. 593–597, DOI: 10.1007/978-0-387-73003-5_160.
[190] H.A. Ignatious, H. El-Sayed, P. Kulkarni, Multilevel data and decision fusion using heterogeneous sensory data for autonomous vehicles, Remote Sens. 15 (9) (2023), [DOI: 10.3390/rs15092256](http://dx.doi.org/10.3390/rs15092256), URL.
[191] Y. Cimtay, E. Ekmekcioglu, S. Caglar-Ozhan, Cross-subject multimodal emotion recognition based on hybrid fusion, IEEE Access 8 (2020) 168865–168878, [DOI: 10.1109/ACCESS.2020.3023871](http://dx.doi.org/10.1109/ACCESS.2020.3023871).
[192] Y. Tan, Z. Sun, F. Duan, J. Solé-Casals, C.F. Caiafa, A multimodal emotion recognition method based on facial expressions and electroencephalography, Biomed. Signal Process. Control 70 (2021) 103029, DOI: 10.1016/j.bspc.2021.103029, URL.
[193] S.K. Khare, U.R. Acharya, Adazd-Net: Automated adaptive and explainable Alzheimer’s disease detection system using EEG signals, Knowl.-Based Syst. (2023) 110858, [DOI: 10.1016/j.knosys.2023.110858](http://dx.doi.org/10.1016/j.knosys.2023.110858), [URL](https://www.sciencedirect.com/science/article/pii/S0950705123006081).
[194] M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu, M. Ghavamzadeh, P. Fieguth, X. Cao, A. Khosravi, U.R. Acharya, et al., A review of uncertainty quantification in deep learning: Techniques, applications and challenges, Inf. Fusion 76 (2021) 243–297.
[195] R. Alizadehsani, M. Roshanzamir, S. Hussain, A. Khosravi, A. Koohestani, M.H. Zangooei, M. Abdar, A. Beykikhoshk, A. Shoeibi, A. Zare, et al., Handling of uncertainty in medical data using machine learning and probability theory techniques: A review of 30 years (1991–2020), Ann. Oper. Res. (2021) 1–42.
[196] S. Seoni, V. Jahmunah, M. Salvi, P.D. Barua, F. Molinari, U.R. Acharya, Application of uncertainty quantification to artificial intelligence in healthcare: A review of last decade (2013–2023), Comput. Biol. Med. (2023) 107441, [DOI: 10.1016/j.compbiomed.2023.107441](http://dx.doi.org/10.1016/j.compbiomed.2023.107441), [URL](https://www.sciencedirect.com/science/article/pii/S001048252300906X).
[197] G. Dandy, W. Wu, A. Simpson, M. Leonard, A review of sources of uncertainty in optimization objectives of water distribution systems, Water 15 (1) (2023), [DOI: 10.3390/w15010136](http://dx.doi.org/10.3390/w15010136), URL.
[198] V. Jahmunah, E. Ng, R.-S. Tan, S.L. Oh, U.R. Acharya, Uncertainty quantification in DenseNet model using myocardial infarction ECG signals, Comput. Methods Programs Biomed. 229 (2023) 107308.
[199] S. Taran, V. Bajaj, Emotion recognition from single-channel EEG signals using a two-stage correlation and instantaneous frequency-based filtering method, Comput. Methods Programs Biomed. 173 (2019) 157–165, DOI: 10.1016/j.cmpb.2019.03.015, URL.
[200] R. Jenke, A. Peer, M. Buss, Feature extraction and selection for emotion recognition from EEG, IEEE Trans. Affect. Comput. 5 (3) (2014) 327–339, http://dx.doi.org/10.1109/TAFFC.2014.2339834.
[201] X. Li, D. Song, P. Zhang, Y. Zhang, Y. Hou, B. Hu, Exploring EEG features in cross-subject emotion recognition, Front. Neurosci. 12 (2018) http://dx.doi.org/ 10.3389/fnins.2018.00162, URL https://www.frontiersin.org/articles/10.3389/ fnins.2018.00162.
[202] H. Chao, L. Dong, Y. Liu, B. Lu, Emotion recognition from multiband EEG signals using CapsNet, Sensors 19 (9) (2019) http://dx.doi.org/10.3390/ s19092212, URL https://www.mdpi.com/1424- 8220/19/9/2212.
[203] X. Xing, Z. Li, T. Xu, L. Shu, B. Hu, X. Xu, SAE+LSTM: A new frame- work for emotion recognition from multi-channel EEG, Front. Neurorobot. 13 (2019) http://dx.doi.org/10.3389/fnbot.2019.00037, URL https://www. frontiersin.org/articles/10.3389/fnbot.2019.00037.
[204] T. Song, W. Zheng, P. Song, Z. Cui, EEG emotion recognition using dynamical graph convolutional neural networks, IEEE Trans. Affect. Comput. 11 (3) (2020) 532–541, http://dx.doi.org/10.1109/TAFFC.2018.2817622.
[205] Y. Cimtay, E. Ekmekcioglu, Investigating the use of pretrained convolutional neural network on cross-subject and cross-dataset EEG emotion recognition, Sensors 20 (7) (2020) http://dx.doi.org/10.3390/s20072034, URL https:// www.mdpi.com/1424- 8220/20/7/2034.
[206] P. Zhong, D. Wang, C. Miao, EEG-based emotion recognition using regularized graph neural networks, IEEE Trans. Affect. Comput. 13 (3) (2022) 1290–1301, http://dx.doi.org/10.1109/TAFFC.2020.2994159.
[207] W.-L. Zheng, B.-L. Lu, Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks, IEEE Trans. Auton. Ment. Dev. 7 (3) (2015) 162–175, http://dx.doi.org/10.1109/TAMD. 2015.2431497.
[208] J.X. Chen, P.W. Zhang, Z.J. Mao, Y.F. Huang, D.M. Jiang, Y.N. Zhang, Accurate EEG-based emotion recognition on combined features using deep convolutional neural networks, IEEE Access 7 (2019) 44317–44328, http://dx.doi.org/10. 1109/ACCESS.2019.2908285.
[209] J. Cheng, M. Chen, C. Li, Y. Liu, R. Song, A. Liu, X. Chen, Emotion recognition from multi-channel EEG via deep forest, IEEE J. Biomed. Health Inf. 25 (2) (2021) 453–464, http://dx.doi.org/10.1109/JBHI.2020.2995767.
[210] W. Tao, C. Li, R. Song, J. Cheng, Y. Liu, F. Wan, X. Chen, EEG-based emotion recognition via channel-wise attention and self attention, IEEE Trans. Affect. Comput. 14 (1) (2023) 382–393, http://dx.doi.org/10.1109/TAFFC. 2020.3025777.
[211] F. Yang, X. Zhao, W. Jiang, P. Gao, G. Liu, Multi-method fusion of cross-subject emotion recognition based on high-dimensional EEG features, Front. Comput. Neurosci. 13 (2019) http://dx.doi.org/10.3389/fncom.2019.00053, URL https: //www.frontiersin.org/articles/10.3389/fncom.2019.00053.
[212] Q. Gao, C.-h. Wang, Z. Wang, X.-l. Song, E.-z. Dong, Y. Song, EEG based emotion recognition using fusion feature extraction method, Multimedia Tools Appl. 79 (2020) 27057–27074.
[213] Y. Peng, F. Qin, W. Kong, Y. Ge, F. Nie, A. Cichocki, GFIL: A unified framework for the importance analysis of features, frequency bands, and channels in EEG- based emotion recognition, IEEE Trans. Cogn. Dev. Syst. 14 (3) (2022) 935–947, http://dx.doi.org/10.1109/TCDS.2021.3082803.
[214] R. Nawaz, K.H. Cheah, H. Nisar, V.V. Yap, Comparison of different feature extraction methods for EEG-based emotion recognition, Biocybern. Biomed. Eng. 40 (3) (2020) 910–926, http://dx.doi.org/10.1016/j.bbe.2020.04.005, URL https://www.sciencedirect.com/science/article/pii/S0208521620300553.
[215] A. Mert, A. Akan, Emotion recognition from EEG signals by using multivariate empirical mode decomposition, Pattern Anal. Appl. 21 (2018) 81–89.
[216] J. Zhang, M. Chen, S. Zhao, S. Hu, Z. Shi, Y. Cao, ReliefF-based EEG sensor selection methods for emotion recognition, Sensors 16 (10) (2016) http://dx. doi.org/10.3390/s16101558, URL https://www.mdpi.com/1424-8220/16/10/ 1558.
[217] D. Maheshwari, S. Ghosh, R. Tripathy, M. Sharma, U.R. Acharya, Automated accurate emotion recognition system using rhythm-specific deep convolutional neural network technique with multi-channel EEG signals, Comput. Biol. Med. 134 (2021) 104428, http://dx.doi.org/10.1016/j.compbiomed.2021.104428, URL https://www.sciencedirect.com/science/article/pii/S0010482521002225.
[218] H. Uyanık, S.T.A. Ozcelik, Z.B. Duranay, A. Sengur, U.R. Acharya, Use of differential entropy for automated emotion recognition in a virtual re- ality environment with EEG signals, Diagnostics 12 (10) (2022) http:// dx.doi.org/10.3390/diagnostics12102508, URL https://www.mdpi.com/2075- 4418/12/10/2508.
[219] M.B.H. Wiem, Z. Lachiri, Emotion classification in arousal valence model using MAHNOB-HCI database, Int. J. Adv. Comput. Sci. Appl. 8 (3) (2017).
[220] Z. Wang, X. Zhou, W. Wang, C. Liang, Emotion recognition using multimodal deep learning in multiple psychophysiological signals and video, Int. J. Mach. Learn. Cybern. 11 (4) (2020) 923–934.
[221] D.B. Setyohadi, S. Kusrohmaniah, S.B. Gunawan, P. Pranowo, Galvanic skin response data classification for emotion detection, Int. J. Electr. Comput. Eng. (IJECE) 8 (5) (2018) 31–41.
[222] S. Dutta, B.K. Mishra, A. Mitra, A. Chakraborty, An analysis of emotion recognition based on GSR signal, ECS Trans. 107 (1) (2022) 12535.
[223] J.Z. Lim, J. Mountstephens, J. Teo, Exploring pupil position as an eye-tracking feature for four-class emotion classification in VR, J. Phys. Conf. Ser. 2129 (1) (2021) 012069, DOI: 10.1088/1742-6596/2129/1/012069.
[224] P. Tarnowski, M. Kołodziej, A. Majkowski, R. Rak, Eye-tracking analysis for emotion recognition, Comput. Intell. Neurosci. 2020 (2020) 1–13, DOI: 10.1155/2020/2909267.
[225] Q. Wu, N. Dey, F. Shi, R.G. Crespo, R.S. Sherratt, Emotion classification on eye-tracking and electroencephalograph fused signals employing deep gradient neural networks, Appl. Soft Comput. 110 (2021) 107752, DOI: 10.1016/j.asoc.2021.107752, URL.
[226] S. Demircan, H. Kahramanli Örnek, Feature extraction from speech data for emotion recognition, J. Adv. Comput. Netw. 2 (2014) 28–30, DOI: 10.7763/JACN.2014.V2.76.
[227] L. Sun, B. Zou, S. Fu, J. Chen, F. Wang, Speech emotion recognition based on DNN-decision tree SVM model, Speech Commun. 115 (2019) 29–37, DOI: 10.1016/j.specom.2019.10.004, URL.
[228] P. Krishnan, A.N. Joseph Raj, V. R, Emotion classification from speech signal based on empirical mode decomposition and non-linear features, Complex Intell. Syst. 7 (2021) 1919–1934, [DOI: 10.1007/s40747-021-00295-z](http://dx.doi.org/10.1007/s40747-021-00295-z).
[229] H.M. Fayek, M. Lech, L. Cavedon, Evaluating deep learning architectures for speech emotion recognition, Neural Netw. 92 (2017) 60–68, DOI: 10.1016/j.neunet.2017.02.013, Advances in Cognitive Engineering Using Neural Networks. URL.
[230] D. Tanko, S. Dogan, F. Burak Demir, M. Baygin, S. Engin Sahin, T. Tuncer, Shoelace pattern-based speech emotion recognition of the lecturers in distance education: ShoePat23, Appl. Acoust. 190 (2022) 108637, DOI: 10.1016/j.apacoust.2022.108637, URL.
[231] Z.-T. Liu, M. Wu, W.-H. Cao, J.-W. Mao, J.-P. Xu, G.-Z. Tan, Speech emotion recognition based on feature selection and extreme learning machine decision tree, Neurocomputing 273 (2018) 271–280, DOI: 10.1016/j.neucom.2017.07.050, URL.
[232] T. Tuncer, S. Dogan, U.R. Acharya, Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques, Knowl.-Based Syst. 211 (2021) 106547, DOI: 10.1016/j.knosys.2020.106547, URL.
[233] Z. ullah, L. Qi, D. Binu, B.R. Rajakumar, B. Mohammed Ismail, 2-D canonical correlation analysis based image super-resolution scheme for facial emotion recognition, Multimedia Tools Appl. 81 (10) (2022) 13911–13934, DOI: 10.1007/s11042-022-11922-3.
[234] H. Li, H. Xu, Deep reinforcement learning for robust emotional classification in facial expression recognition, Knowl.-Based Syst. 204 (2020) 106172, DOI: 10.1016/j.knosys.2020.106172, URL.
[235] K. Chowdary, T. Nguyen, D. Hemanth, Deep learning-based facial emotion recognition for human–computer interaction applications, Neural Comput. Appl. (2021) 1–18, [DOI: 10.1007/s00521-021-06012-8](http://dx.doi.org/10.1007/s00521-021-06012-8).
[236] D.K. Jain, P. Shamsolmoali, P. Sehdev, Extended deep neural network for facial emotion recognition, Pattern Recognit. Lett. 120 (2019) 69–74, [DOI: 10.1016/j.patrec.2019.01.008](http://dx.doi.org/10.1016/j.patrec.2019.01.008), URL.
[237] F. Zhang, T. Zhang, Q. Mao, C. Xu, Geometry guided pose-invariant facial expression recognition, IEEE Trans. Image Process. 29 (2020) 4445–4460, [DOI: 10.1109/TIP.2020.2972114](http://dx.doi.org/10.1109/TIP.2020.2972114).
[238] H. Zhang, A. Jolfaei, M. Alazab, A face emotion recognition method using convolutional neural network and image edge computing, IEEE Access 7 (2019) 159081–159089, [DOI: 10.1109/ACCESS.2019.2949741](http://dx.doi.org/10.1109/ACCESS.2019.2949741).
[239] N.D. Mehendale, Facial emotion recognition using convolutional neural networks (FERC), SN Appl. Sci. 2 (2020) 1–8.
[240] R. Kumar, S. Muniasamy, N. Arumugam, Facial emotion recognition using subband selective multilevel stationary wavelet gradient transform and fuzzy support vector machine, Vis. Comput. 37 (2021) 1–15, DOI: 10.1007/s00371-020-01988-1.
[241] Y.-D. Zhang, Z.-J. Yang, H.-M. Lu, X.-X. Zhou, P. Phillips, Q.-M. Liu, S.-H. Wang, Facial emotion recognition based on biorthogonal wavelet entropy, fuzzy support vector machine, and stratified cross validation, IEEE Access 4 (2016) 8375–8385, [DOI: 10.1109/ACCESS.2016.2628407](http://dx.doi.org/10.1109/ACCESS.2016.2628407).
[242] K. Sarvakar, R. Senkamalavalli, S. Raghavendra, J. Santosh Kumar, R. Manjunath, S. Jaiswal, Facial emotion recognition using convolutional neural networks, Mater. Today: Proc. 80 (2023) 3560–3564, DOI: 10.1016/j.matpr.2021.07.297, SI:5 NANO 2021. URL.
[243] S. Zhao, H. Yao, Y. Gao, R. Ji, G. Ding, Continuous probability distribution prediction of image emotions via multitask shared sparse regression, IEEE Trans. Multimed. 19 (3) (2017) 632–645, DOI: 10.1109/TMM.2016.2617741.
[244] S. Katsigiannis, N. Ramzan, DREAMER: A database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices, IEEE J. Biomed. Health Inf. 22 (1) (2018) 98–107, DOI: 10.1109/JBHI.2017.2688239.
[245] R.-N. Duan, J.-Y. Zhu, B.-L. Lu, Differential entropy feature for EEG-based emotion classification, in: 6th International IEEE/EMBS Conference on Neural Engineering (NER), IEEE, 2013, pp. 81–84.
[246] S. Sangnark, P. Autthasan, P. Ponglertnapakorn, P. Chalekarn, T. Sudhawiyangkul, M. Trakulruangroj, S. Songsermsawad, R. Assabumrungrat, S. Amplod, K. Ounjai, T. Wilaiprasitporn, Revealing preference in popular music through familiarity and brain response, IEEE Sens. J. (2021) 1.
[247] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi, T. Pun, A. Nijholt, I. Patras, DEAP: A database for emotion analysis using physiological signals, IEEE Trans. Affect. Comput. 3 (1) (2012) 18–31, DOI: 10.1109/T-AFFC.2011.15.
[248] P. Lakhan, N. Banluesombatkul, V. Changniam, R. Dhithijaiyratn, P. Leelaarporn, E. Boonchieng, S. Hompoonsup, T. Wilaiprasitporn, Consumer grade brain sensing for emotion recognition, IEEE Sens. J. 19 (21) (2019) 9896–9907, [DOI: 10.1109/JSEN.2019.2928781](http://dx.doi.org/10.1109/JSEN.2019.2928781).
[249] E. Ekmekcioglu, Y. Cimtay, Loughborough university multimodal emotion dataset-2, 2021, [DOI: 10.6084/m9.figshare.12644033.v5](http://dx.doi.org/10.6084/m9.figshare.12644033.v5), [URL](https://figshare.com/articles/dataset/Loughborough_University_Multimodal_Emotion_Dataset_- _2/12644033).
[250] W. Zheng, W. Liu, Y. Lu, B. Lu, A. Cichocki, EmotionMeter: A multimodal framework for recognizing human emotions, IEEE Trans. Cybern. (2018) 1–13, [DOI: 10.1109/TCYB.2018.2797176](http://dx.doi.org/10.1109/TCYB.2018.2797176).
[251] M. Soleymani, J. Lichtenauer, T. Pun, M. Pantic, A multimodal database for affect recognition and implicit tagging, IEEE Trans. Affect. Comput. 3 (1) (2012) 42–55, [DOI: 10.1109/T-AFFC.2011.25](http://dx.doi.org/10.1109/T-AFFC.2011.25).
[252] G. Zhao, Y. Zhang, Y. Ge, Y. Zheng, X. Sun, K. Zhang, Asymmetric hemisphere activation in tenderness: evidence from EEG signals, Sci. Rep. 8 (1) (2018) 8029.
[253] G. Zhao, Y. Zhang, Y. Ge, Frontal EEG asymmetry and middle line power difference in discrete emotions, Front. Behav. Neurosci. 12 (2018) 225.
[254] J.A. Miranda-Correa, M.K. Abadi, N. Sebe, I. Patras, AMIGOS: A dataset for affect, personality and mood research on individuals and groups, IEEE Trans. Affect. Comput. 12 (2) (2021) 479–493, DOI: 10.1109/TAFFC.2018.2884461.
[255] T.B. Alakus, M. Gonen, I. Turkoglu, Database for an emotion recognition system based on EEG signals and various computer games - GAMEEMO, Biomed. Signal Process. Control 60 (2020) 101951, DOI: 10.1016/j.bspc.2020.101951, URL.
[256] T. Song, W. Zheng, C. Lu, Y. Zong, X. Zhang, Z. Cui, MPED: A multi-modal physiological emotion database for discrete emotion recognition, IEEE Access 7 (2019) 12177–12191, [DOI: 10.1109/ACCESS.2019.2891579](http://dx.doi.org/10.1109/ACCESS.2019.2891579).
[257] R. Subramanian, J. Wache, M.K. Abadi, R.L. Vieriu, S. Winkler, N. Sebe, ASCERTAIN: emotion and personality recognition using commercial sensors, IEEE Trans. Affect. Comput. 9 (2) (2018) 147–160, DOI: 10.1109/TAFFC.2016.2625250.
[258] A. Baghdadi, Y. Aribi, R. Fourati, N. Halouani, P. Siarry, A.M. Alimi, DASPS: A database for anxious states based on a psychological stimulation, 2019, arXiv:1901.02942.
[259] M. Yu, S. Xiao, M. Hua, H. Wang, X. Chen, F. Tian, Y. Li, EEG-based emotion recognition in an immersive virtual reality environment: From local activity to brain network features, Biomed. Signal Process. Control 72 (2022) 103349, [DOI: 10.1016/j.bspc.2021.103349](http://dx.doi.org/10.1016/j.bspc.2021.103349), URL.
[260] R. Ivanov, F. Kazantsev, E. Zavarzin, A. Klimenko, N. Milakhina, Y.G. Matushkin, A. Savostyanov, S. Lashin, ICBrainDB.: an integrated database for finding associations between genetic factors and EEG markers of depressive disorders, J. Pers. Med. 12 (1) (2022) 53.
[261] J. Wagner, J. Kim, E. Andre, From physiological signals to emotions: Implementing and comparing selected methods for feature extraction and classification, in: 2005 IEEE International Conference on Multimedia and Expo, 2005, pp. 940–943, [DOI: 10.1109/ICME.2005.1521579](http://dx.doi.org/10.1109/ICME.2005.1521579).
[262] P. Schmidt, A. Reiss, R. Duerichen, C. Marberger, K. Van Laerhoven, Introducing WESAD, a multimodal dataset for wearable stress and affect detection, in: Proceedings of the 20th ACM International Conference on Multimodal Interaction, ICMI ’18, Association for Computing Machinery, New York, NY, USA, 2018, pp. 400–408, [DOI: 10.1145/3242969.3242985](http://dx.doi.org/10.1145/3242969.3242985).
[263] L. Zhang, S. Walter, X. Ma, P. Werner, A. Al-Hamadi, H.C. Traue, S. Gruss, ‘‘BioVid Emo DB’’: A multimodal database for emotion analyses validated by subjective ratings, in: 2016 IEEE Symposium Series on Computational Intelligence (SSCI), 2016, pp. 1–6, DOI: 10.1109/SSCI.2016.7849931.
[264] S. Koldijk, M. Sappelli, S. Verberne, M.A. Neerincx, W. Kraaij, The SWELL knowledge work dataset for stress and user modeling research, in: Proceedings of the 16th International Conference on Multimodal Interaction, ICMI ’14, Association for Computing Machinery, New York, NY, USA, 2014, pp. 291–298, [DOI: 10.1145/2663204.2663257](http://dx.doi.org/10.1145/2663204.2663257).
[265] J.-H. Maeng, D.-H. Kang, D.-H. Kim, Deep learning method for selecting effective models and feature groups in emotion recognition using an Asian multimodal database, Electronics 9 (12) (2020), DOI: 10.3390/electronics9121988, URL.
[266] S.R. Livingstone, F.A. Russo, The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English, PLOS ONE 13 (5) (2018) 1–35, [DOI: 10.1371/journal.pone.0196391](http://dx.doi.org/10.1371/journal.pone.0196391).
[267] E. Chen, Z. Lu, H. Xu, L. Cao, Y. Zhang, J. Fan, A large scale speech sentiment corpus, in: Proceedings of the Twelfth Language Resources and Evaluation Conference, European Language Resources Association, Marseille, France, 2020, pp. 6549–6555, [URL](https://aclanthology.org/2020.lrec-1.806).
[268] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J.N. Chang, S. Lee, S.S. Narayanan, IEMOCAP: Interactive emotional dyadic motion capture database, Lang. Resour. Eval. 42 (2008) 335–359.
[269] F. Burkhardt, A. Paeschke, M. Rolfes, W.F. Sendlmeier, B. Weiss, et al., A database of German emotional speech, in: Interspeech, Vol. 5, 2005, pp. 1517–1520.
[270] A. Dhall, R. Goecke, S. Lucey, T. Gedeon, Collecting large, richly annotated facial-expression databases from movies, IEEE MultiMedia 19 (3) (2012) 34–41, [DOI: 10.1109/MMUL.2012.26](http://dx.doi.org/10.1109/MMUL.2012.26).
[271] W. Bao, Y. Li, M. Gu, M. Yang, H. Li, L. Chao, J. Tao, Building a Chinese natural emotional audio-visual database, in: 2014 12th International Conference on Signal Processing (ICSP), 2014, pp. 583–587, DOI: 10.1109/ICOSP.2014.7015071.
[272] K. Wang, Q. Zhang, S. Liao, A database of elderly emotional speech, in: Proc. Int. Symp. Signal Process. Biomed. Eng Informat, 2014, pp. 549–553.
[273] S.G. Koolagudi, R. Reddy, J. Yadav, K.S. Rao, IITKGP-SEHSC : Hindi speech corpus for emotion analysis, in: 2011 International Conference on Devices and Communications (ICDeCom), 2011, pp. 1–5, DOI: 10.1109/ICDECOM.2011.5738540.
[274] O. Martin, I. Kotsia, B. Macq, I. Pitas, The eNTERFACE’ 05 audio-visual emotion database, in: 22nd International Conference on Data Engineering Workshops (ICDEW’06), 2006, p. 8, [DOI: 10.1109/ICDEW.2006.145](http://dx.doi.org/10.1109/ICDEW.2006.145).
[275] T. Bänziger, M. Mortillaro, K.R. Scherer, Introducing the Geneva Multimodal expression corpus for experimental research on emotion perception, Emotion 12 (5) (2012) 1161.
[276] S. Haq, P. Jackson, Speaker-dependent audio-visual emotion recognition, in: Proc. Int. Conf. on Auditory-Visual Speech Processing (AVSP’08), Norwich, UK, 2009.
[277] S. Haq, P. Jackson, in: W. Wang (Ed.), Machine Audition: Principles, Algorithms and Systems, IGI Global, Hershey PA, 2010, pp. 398–423, Ch. Multimodal Emotion Recognition.
[278] S. Haq, P. Jackson, J. Edge, Audio-visual feature selection and reduction for emotion classification, in: Proc. Int. Conf. on Auditory-Visual Speech Processing (AVSP’08), Tangalooma, Australia, 2008.
[279] A. Batliner, S. Steidl, E. Nöth, Releasing a thoroughly annotated and processed spontaneous emotional database: the FAU Aibo Emotion Corpus, 2008.
[280] M.K. Pichora-Fuller, K. Dupuis, Toronto emotional speech set (TESS), 2020, [DOI: 10.5683/SP2/E8H2MF](http://dx.doi.org/10.5683/SP2/E8H2MF).
[281] S. Zhalehpour, O. Onder, Z. Akhtar, C.E. Erdem, BAUM-1: A spontaneous audio-visual face database of affective and mental states, IEEE Trans. Affect. Comput. 8 (3) (2017) 300–313, [DOI: 10.1109/TAFFC.2016.2553038](http://dx.doi.org/10.1109/TAFFC.2016.2553038).
[282] Y. Wang, L. Guan, Recognizing human emotional state from audiovisual signals, IEEE Trans. Multimed. 10 (5) (2008) 936–946, DOI: 10.1109/TMM.2008.927665.
[283] G. Costantini, I. Iaderola, A. Paoloni, M. Todisco, EMOVO corpus: an Italian emotional speech database, in: Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), European Language Resources Association (ELRA), Reykjavik, Iceland, 2014, pp. 3501–3504, URL.
[284] D. Lundqvist, A. Flykt, A. Öhman, Karolinska directed emotional faces, Cogn. Emot. (1998).
[285] P. Lucey, J.F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, I. Matthews, The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression, in: 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, 2010, pp. 94–101, [DOI: 10.1109/CVPRW.2010.5543262](http://dx.doi.org/10.1109/CVPRW.2010.5543262).
[286] M. Lyons, S. Akamatsu, M. Kamachi, J. Gyoba, Coding facial expressions with Gabor wavelets, in: Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition, 1998, pp. 200–205, DOI: 10.1109/AFGR.1998.670949.
[287] R. Gross, I. Matthews, J. Cohn, T. Kanade, S. Baker, Multi-PIE, in: 2008 8th IEEE International Conference on Automatic Face & Gesture Recognition, 2008, pp. 1–8, [DOI: 10.1109/AFGR.2008.4813399](http://dx.doi.org/10.1109/AFGR.2008.4813399).
[288] R. Gross, I. Matthews, J. Cohn, T. Kanade, S. Baker, Multi-PIE, Image Vis. Comput. 28 (5) (2010) 807–813, DOI: 10.1016/j.imavis.2009.08.002, Best of Automatic Face and Gesture Recognition 2008. [URL](https://www.sciencedirect.com/science/article/pii/S0262885609001711).
[289] A. Mollahosseini, B. Hasani, M.H. Mahoor, AffectNet: A database for facial expression, valence, and arousal computing in the wild, IEEE Trans. Affect. Comput. 10 (1) (2019) 18–31, DOI: 10.1109/TAFFC.2017.2740923.
[290] I.J. Goodfellow, D. Erhan, P.L. Carrier, A. Courville, M. Mirza, B. Hamner, W. Cukierski, Y. Tang, D. Thaler, D.-H. Lee, et al., Challenges in representation learning: A report on three machine learning contests, in: Neural Information Processing: 20th International Conference, ICONIP 2013, Daegu, Korea, November 3-7, 2013. Proceedings, Part III 20, Springer, 2013, pp. 117–124.
[291] M. Pantic, M. Valstar, R. Rademaker, L. Maat, Web-based database for facial expression analysis, in: 2005 IEEE International Conference on Multimedia and Expo, 2005, p. 5, [DOI: 10.1109/ICME.2005.1521424](http://dx.doi.org/10.1109/ICME.2005.1521424).
[292] S. Li, W. Deng, Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 2017, pp. 2584–2593. S. Li, W. Deng, Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition, IEEE Trans. Image Process. 28 (1) (2019) 356–370.
[293] Z. Zhang, P. Luo, C.C. Loy, X. Tang, From facial expression recognition to interpersonal relation prediction, Int. J. Comput. Vis. 126 (2018) 550–569.
[294] N. Aifanti, C. Papachristou, A. Delopoulos, The MUG facial expression database, in: 11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10, 2010, pp. 1–4.
[295] D. Aneja, A. Colburn, G. Faigin, L. Shapiro, B. Mones, Modeling stylized character expressions via deep learning, in: Computer Vision–ACCV 2016: 13th Asian Conference on Computer Vision, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13, Springer, 2017, pp. 136–153.
[296] A. Dhall, R. Goecke, S. Lucey, T. Gedeon, Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark, in: 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), 2011, pp. 2106–2112, [DOI: 10.1109/ICCVW.2011.6130508](http://dx.doi.org/10.1109/ICCVW.2011.6130508).
[298] L. Yin, X. Wei, Y. Sun, J. Wang, M. Rosato, A 3D facial expression database for facial behavior research, in: 7th International Conference on Automatic Face and Gesture Recognition (FGR06), 2006, pp. 211–216, http://dx.doi.org/10. 1109/FGR.2006.6.
[299] G.B. Huang, M. Mattar, T. Berg, E. Learned-Miller, Labeled faces in the wild: A database forstudying face recognition in unconstrained environments, in: Workshop on Faces in’Real-Life’Images: Detection, Alignment, and Recognition, 2008.
[300] G. Zhao, X. Huang, M. Taini, S.Z. Li, M. Pietikäinen, Facial expression recognition from near-infrared videos, Image Vis. Comput. 29 (9) (2011) 607–619, http://dx.doi.org/10.1016/j.imavis.2011.07.002, URL https://www. sciencedirect.com/science/article/pii/S0262885611000515.
[301] C.I. Watson, NIST special database 18. NIST Mugshot Identification Database (MID), 2008.
[302] F. Wallhoff, B. Schuller, M. Hawellek, G. Rigoll, Efficient recognition of authentic dynamic facial expressions on the feedtum database, in: 2006 IEEE International Conference on Multimedia and Expo, 2006, pp. 493–496, http: //dx.doi.org/10.1109/ICME.2006.262433.
[303] Q. You, J. Luo, H. Jin, J. Yang, Building a large scale dataset for image emotion recognition: The fine print and the benchmark, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 30, 2016.
[304] O. Langner, R. Dotsch, G. Bijlstra, D.H. Wigboldus, S.T. Hawk, A. Van Knippen- berg, Presentation and validation of the Radboud Faces Database, Cogn. Emot. 24 (8) (2010) 1377–1388.
[305] P. Lucey, J.F. Cohn, K.M. Prkachin, P.E. Solomon, I. Matthews, Painful data: The UNBC-McMaster shoulder pain expression archive database, in: 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG), 2011, pp. 57–64, http://dx.doi.org/10.1109/FG.2011.5771462.
